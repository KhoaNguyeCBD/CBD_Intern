{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# <center> Author Classification </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using NLP and techniques to classify author from texts from Gutenberg project.\n",
    "1. Pre-process data using Spacy and other methods.\n",
    "2. Perform data exploration\n",
    "3. Using Bag of Word, apply supervised models such as Naive Bayes,  Decision Tree, Random Forest, and Gradient Boosting.\n",
    "4. Similar to 3., but using TF-IDF.\n",
    "5. Similar to 3., but using word2vec."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Introduction\" data-toc-modified-id=\"Introduction-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Introduction</a></span></li><li><span><a href=\"#Explore-Data\" data-toc-modified-id=\"Explore-Data-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Explore Data</a></span></li><li><span><a href=\"#Prepare-Data\" data-toc-modified-id=\"Prepare-Data-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Prepare Data</a></span></li><li><span><a href=\"#Bag-of-words\" data-toc-modified-id=\"Bag-of-words-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Bag of words</a></span></li><li><span><a href=\"#TF-IDF\" data-toc-modified-id=\"TF-IDF-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>TF-IDF</a></span></li><li><span><a href=\"#Word2vec\" data-toc-modified-id=\"Word2vec-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Word2vec</a></span></li><li><span><a href=\"#LDA\" data-toc-modified-id=\"LDA-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>LDA</a></span></li><li><span><a href=\"#Conclusion\" data-toc-modified-id=\"Conclusion-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Conclusion</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "colab_type": "code",
    "id": "ME2U1pNSNdhU",
    "outputId": "1b946645-2565-4ad4-c45e-5939defc6d4f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 276,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import gutenberg\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from itertools import chain\n",
    "\n",
    "nltk.download('gutenberg')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P5YjuGJEmc__"
   },
   "source": [
    "https://machinelearningmastery.com/prepare-text-data-machine-learning-scikit-learn/\n",
    "\n",
    "https://www.kaggle.com/c/word2vec-nlp-tutorial/overview/part-3-more-fun-with-word-vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 341
    },
    "colab_type": "code",
    "id": "-ibYL6h7N81Q",
    "outputId": "199cb6eb-a8b0-41db-851c-29cdb293173b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['austen-emma.txt',\n",
       " 'austen-persuasion.txt',\n",
       " 'austen-sense.txt',\n",
       " 'bible-kjv.txt',\n",
       " 'blake-poems.txt',\n",
       " 'bryant-stories.txt',\n",
       " 'burgess-busterbrown.txt',\n",
       " 'carroll-alice.txt',\n",
       " 'chesterton-ball.txt',\n",
       " 'chesterton-brown.txt',\n",
       " 'chesterton-thursday.txt',\n",
       " 'edgeworth-parents.txt',\n",
       " 'melville-moby_dick.txt',\n",
       " 'milton-paradise.txt',\n",
       " 'shakespeare-caesar.txt',\n",
       " 'shakespeare-hamlet.txt',\n",
       " 'shakespeare-macbeth.txt',\n",
       " 'whitman-leaves.txt']"
      ]
     },
     "execution_count": 277,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Novels = gutenberg.fileids()\n",
    "Novels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is name of author followed title of the book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "75XdoSG-OFha"
   },
   "outputs": [],
   "source": [
    "numNovels = len(gutenberg.fileids())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 18 book in this project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 251
    },
    "colab_type": "code",
    "id": "cUwjMei6PR5j",
    "outputId": "10b24789-92c8-4dbf-edd7-f9f9fd72439e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['austen',\n",
       " 'bible',\n",
       " 'blake',\n",
       " 'bryant',\n",
       " 'burgess',\n",
       " 'carroll',\n",
       " 'chesterton',\n",
       " 'edgeworth',\n",
       " 'melville',\n",
       " 'milton',\n",
       " 'shakespeare',\n",
       " 'whitman']"
      ]
     },
     "execution_count": 279,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Authors = []\n",
    "for i in range(numNovels):\n",
    "  author = Novels[i].split('-')[0]\n",
    "  if  (author in Authors ):\n",
    "    continue\n",
    "  Authors.append(Novels[i].split('-')[0])\n",
    "print(len(Authors))\n",
    "Authors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 12 authors who wrote 18 books above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 341
    },
    "colab_type": "code",
    "id": "EX7WH-vxPVCy",
    "outputId": "c9ddd323-e56a-4fd9-c9c0-6ff9124d9447"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "austen-emma has 192427 words\n",
      "austen-persuasion has 98171 words\n",
      "austen-sense has 141576 words\n",
      "bible-kjv has 1010654 words\n",
      "blake-poems has 8354 words\n",
      "bryant-stories has 55563 words\n",
      "burgess-busterbrown has 18963 words\n",
      "carroll-alice has 34110 words\n",
      "chesterton-ball has 96996 words\n",
      "chesterton-brown has 86063 words\n",
      "chesterton-thursday has 69213 words\n",
      "edgeworth-parents has 210663 words\n",
      "melville-moby_dick has 260819 words\n",
      "milton-paradise has 96825 words\n",
      "shakespeare-caesar has 25833 words\n",
      "shakespeare-hamlet has 37360 words\n",
      "shakespeare-macbeth has 23140 words\n",
      "whitman-leaves has 154883 words\n"
     ]
    }
   ],
   "source": [
    "for i in Novels:\n",
    "  print(i.split('.')[0] + \" has \" + str(len(gutenberg.words(i))) + ' words'  )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results above show total of words in each book."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They will be transformed to dataframe for easier to read, and this data frame sumarize all information about words, senteces and vocalbulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AEoKiS_KYjaF"
   },
   "outputs": [],
   "source": [
    "num_word = []\n",
    "num_sent = []\n",
    "num_vocab = []\n",
    "for fileid in gutenberg.fileids():\n",
    "    num_word.append(len(gutenberg.words(fileid)) )\n",
    "    num_sent.append(len(gutenberg.sents(fileid)) )\n",
    "    num_vocab.append(len(set(gutenberg.words(fileid))) )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JQlwE_l4fo6m"
   },
   "outputs": [],
   "source": [
    "suma = pd.DataFrame( index= Novels, columns = ['Words','Sentences','Vocabulary'], data = np.array([num_word, num_sent,num_vocab]).T )  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 607
    },
    "colab_type": "code",
    "id": "-_FfuEntjFay",
    "outputId": "6cdbdfa4-3712-42ea-d34f-c1ef3277b324"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Words</th>\n",
       "      <th>Sentences</th>\n",
       "      <th>Vocabulary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>austen-emma.txt</th>\n",
       "      <td>192427</td>\n",
       "      <td>7752</td>\n",
       "      <td>7811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>austen-persuasion.txt</th>\n",
       "      <td>98171</td>\n",
       "      <td>3747</td>\n",
       "      <td>6132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>austen-sense.txt</th>\n",
       "      <td>141576</td>\n",
       "      <td>4999</td>\n",
       "      <td>6833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bible-kjv.txt</th>\n",
       "      <td>1010654</td>\n",
       "      <td>30103</td>\n",
       "      <td>13769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>blake-poems.txt</th>\n",
       "      <td>8354</td>\n",
       "      <td>438</td>\n",
       "      <td>1820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bryant-stories.txt</th>\n",
       "      <td>55563</td>\n",
       "      <td>2863</td>\n",
       "      <td>4420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>burgess-busterbrown.txt</th>\n",
       "      <td>18963</td>\n",
       "      <td>1054</td>\n",
       "      <td>1764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>carroll-alice.txt</th>\n",
       "      <td>34110</td>\n",
       "      <td>1703</td>\n",
       "      <td>3016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chesterton-ball.txt</th>\n",
       "      <td>96996</td>\n",
       "      <td>4779</td>\n",
       "      <td>8947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chesterton-brown.txt</th>\n",
       "      <td>86063</td>\n",
       "      <td>3806</td>\n",
       "      <td>8299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chesterton-thursday.txt</th>\n",
       "      <td>69213</td>\n",
       "      <td>3742</td>\n",
       "      <td>6807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>edgeworth-parents.txt</th>\n",
       "      <td>210663</td>\n",
       "      <td>10230</td>\n",
       "      <td>9593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>melville-moby_dick.txt</th>\n",
       "      <td>260819</td>\n",
       "      <td>10059</td>\n",
       "      <td>19317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>milton-paradise.txt</th>\n",
       "      <td>96825</td>\n",
       "      <td>1851</td>\n",
       "      <td>10751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>shakespeare-caesar.txt</th>\n",
       "      <td>25833</td>\n",
       "      <td>2163</td>\n",
       "      <td>3560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>shakespeare-hamlet.txt</th>\n",
       "      <td>37360</td>\n",
       "      <td>3106</td>\n",
       "      <td>5447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>shakespeare-macbeth.txt</th>\n",
       "      <td>23140</td>\n",
       "      <td>1907</td>\n",
       "      <td>4017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>whitman-leaves.txt</th>\n",
       "      <td>154883</td>\n",
       "      <td>4250</td>\n",
       "      <td>14329</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Words  Sentences  Vocabulary\n",
       "austen-emma.txt           192427       7752        7811\n",
       "austen-persuasion.txt      98171       3747        6132\n",
       "austen-sense.txt          141576       4999        6833\n",
       "bible-kjv.txt            1010654      30103       13769\n",
       "blake-poems.txt             8354        438        1820\n",
       "bryant-stories.txt         55563       2863        4420\n",
       "burgess-busterbrown.txt    18963       1054        1764\n",
       "carroll-alice.txt          34110       1703        3016\n",
       "chesterton-ball.txt        96996       4779        8947\n",
       "chesterton-brown.txt       86063       3806        8299\n",
       "chesterton-thursday.txt    69213       3742        6807\n",
       "edgeworth-parents.txt     210663      10230        9593\n",
       "melville-moby_dick.txt    260819      10059       19317\n",
       "milton-paradise.txt        96825       1851       10751\n",
       "shakespeare-caesar.txt     25833       2163        3560\n",
       "shakespeare-hamlet.txt     37360       3106        5447\n",
       "shakespeare-macbeth.txt    23140       1907        4017\n",
       "whitman-leaves.txt        154883       4250       14329"
      ]
     },
     "execution_count": 283,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "suma\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sbc2bslkkAGa"
   },
   "source": [
    "**bible-kjv is the book which has largest amount of words than the others. while blake-poems is the least. It can understand that poems is less words than novels.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now extract an random book to show its content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**=> it is raw data beccasue it has a lot symbol like \\n, ...**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "uoJqkJgyj_8L",
    "outputId": "1c911f6b-3169-4ccb-fb60-9b52ea42eba0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['[', 'Emma', 'by', 'Jane', 'Austen', '1816', ']']], [['VOLUME', 'I']]]"
      ]
     },
     "execution_count": 285,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gutenberg.paras('austen-emma.txt')[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**=>Because the number of sentences are too large, this project focuse on \"paras\" which consider as set of sentences. to reduce the number of samples**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 359
    },
    "colab_type": "code",
    "id": "sOUtQ7fLH77x",
    "outputId": "a988a306-cf8e-461b-cc69-6b04bc10d845"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "austen-emma has 2371 paragraphs\n",
      "austen-persuasion has 1032 paragraphs\n",
      "austen-sense has 1862 paragraphs\n",
      "bible-kjv has 24608 paragraphs\n",
      "blake-poems has 284 paragraphs\n",
      "bryant-stories has 1194 paragraphs\n",
      "burgess-busterbrown has 266 paragraphs\n",
      "carroll-alice has 817 paragraphs\n",
      "chesterton-ball has 1606 paragraphs\n",
      "chesterton-brown has 1161 paragraphs\n",
      "chesterton-thursday has 1288 paragraphs\n",
      "edgeworth-parents has 3726 paragraphs\n",
      "melville-moby_dick has 2793 paragraphs\n",
      "milton-paradise has 29 paragraphs\n",
      "shakespeare-caesar has 744 paragraphs\n",
      "shakespeare-hamlet has 950 paragraphs\n",
      "shakespeare-macbeth has 678 paragraphs\n",
      "whitman-leaves has 2478 paragraphs\n",
      "47887\n"
     ]
    }
   ],
   "source": [
    "s = 0\n",
    "for i in Novels:\n",
    "  print(i.split('.')[0] + \" has \" +  str(len(gutenberg.paras(i)))  + \" paragraphs\")\n",
    "  s = s + len(gutenberg.paras(i))\n",
    "print(s)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Each sample will have 500 paras to reduce the number of samples and process data faster**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "O3vQtFl-ufwh",
    "outputId": "a3e1d276-6fc3-4f1c-c683-0836485a5ee7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blake-poems has 284 paragraphs\n",
      "burgess-busterbrown has 266 paragraphs\n",
      "milton-paradise has 29 paragraphs\n"
     ]
    }
   ],
   "source": [
    "for i in Novels:\n",
    "  if (len(gutenberg.paras(i)) < 500):\n",
    "    print(i.split('.')[0] + \" has \" +  str(len(gutenberg.paras(i)))  + \" paragraphs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2iTGzix0EAiP"
   },
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Rm2v6BG8HFNj"
   },
   "source": [
    "Generate data from the books which has 3 features titles, paras and authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "JdDOE9nrwo_K",
    "outputId": "1bd12f52-b8fe-45c6-ffa2-6ae446a9d57e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.847790479660034\n"
     ]
    }
   ],
   "source": [
    "# Titles, Sentences, Authors\n",
    "Titles = []\n",
    "Paras = []\n",
    "Authors = []\n",
    "import time\n",
    "tick = time.time()\n",
    "# get the data\n",
    "from itertools import chain\n",
    "\n",
    "for fileid in gutenberg.fileids():\n",
    "    author = fileid.split('-')[0] \n",
    "    kk = gutenberg.paras(fileid) \n",
    "    title = fileid.split('-')[1].split('.')[0] \n",
    "    for para in kk:\n",
    "        Authors.append(author)\n",
    "        Titles.append(title)\n",
    "        para = list(chain.from_iterable(para)) \n",
    "        Paras.append(para)\n",
    "    \n",
    "print(time.time() - tick)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "colab_type": "code",
    "id": "K3IwO7JqV404",
    "outputId": "52cc69f2-557b-4d2f-b650-591339ec588e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Titles</th>\n",
       "      <th>Paras</th>\n",
       "      <th>Authors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>emma</td>\n",
       "      <td>[[, Emma, by, Jane, Austen, 1816, ]]</td>\n",
       "      <td>austen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>emma</td>\n",
       "      <td>[VOLUME, I]</td>\n",
       "      <td>austen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>emma</td>\n",
       "      <td>[CHAPTER, I]</td>\n",
       "      <td>austen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>emma</td>\n",
       "      <td>[Emma, Woodhouse, ,, handsome, ,, clever, ,, a...</td>\n",
       "      <td>austen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>emma</td>\n",
       "      <td>[She, was, the, youngest, of, the, two, daught...</td>\n",
       "      <td>austen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47882</th>\n",
       "      <td>leaves</td>\n",
       "      <td>[}, Good, -, Bye, My, Fancy, !]</td>\n",
       "      <td>whitman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47883</th>\n",
       "      <td>leaves</td>\n",
       "      <td>[Good, -, bye, my, Fancy, !, Farewell, dear, m...</td>\n",
       "      <td>whitman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47884</th>\n",
       "      <td>leaves</td>\n",
       "      <td>[Now, for, my, last, --, let, me, look, back, ...</td>\n",
       "      <td>whitman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47885</th>\n",
       "      <td>leaves</td>\n",
       "      <td>[Long, have, we, lived, ,, joy, ', d, ,, cares...</td>\n",
       "      <td>whitman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47886</th>\n",
       "      <td>leaves</td>\n",
       "      <td>[Yet, let, me, not, be, too, hasty, ,, Long, i...</td>\n",
       "      <td>whitman</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>47887 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Titles                                              Paras  Authors\n",
       "0        emma               [[, Emma, by, Jane, Austen, 1816, ]]   austen\n",
       "1        emma                                        [VOLUME, I]   austen\n",
       "2        emma                                       [CHAPTER, I]   austen\n",
       "3        emma  [Emma, Woodhouse, ,, handsome, ,, clever, ,, a...   austen\n",
       "4        emma  [She, was, the, youngest, of, the, two, daught...   austen\n",
       "...       ...                                                ...      ...\n",
       "47882  leaves                    [}, Good, -, Bye, My, Fancy, !]  whitman\n",
       "47883  leaves  [Good, -, bye, my, Fancy, !, Farewell, dear, m...  whitman\n",
       "47884  leaves  [Now, for, my, last, --, let, me, look, back, ...  whitman\n",
       "47885  leaves  [Long, have, we, lived, ,, joy, ', d, ,, cares...  whitman\n",
       "47886  leaves  [Yet, let, me, not, be, too, hasty, ,, Long, i...  whitman\n",
       "\n",
       "[47887 rows x 3 columns]"
      ]
     },
     "execution_count": 289,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataOrig = pd.DataFrame({ 'Titles' : Titles,\n",
    "                      'Paras':    Paras,\n",
    "                      'Authors': Authors})\n",
    "dataOrig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using stop word in english to filter data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mcwYd-JkWFUh"
   },
   "outputs": [],
   "source": [
    "data = dataOrig.copy()\n",
    "stop_words = set(stopwords.words('english')) \n",
    "for i in range(data.shape[0]):\n",
    "  words = ''\n",
    "  for w in data[\"Paras\"][i]:\n",
    "    if not w in stop_words:\n",
    "      words = words + \" \" + w \n",
    "  data[\"Paras\"][i] = words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "kPJFl9CUPjbP",
    "outputId": "23bad7aa-9f08-4dde-92e2-fa60d8da84a3"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Titles</th>\n",
       "      <th>Paras</th>\n",
       "      <th>Authors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>emma</td>\n",
       "      <td>[ Emma Jane Austen 1816 ]</td>\n",
       "      <td>austen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>emma</td>\n",
       "      <td>VOLUME I</td>\n",
       "      <td>austen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>emma</td>\n",
       "      <td>CHAPTER I</td>\n",
       "      <td>austen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>emma</td>\n",
       "      <td>Emma Woodhouse , handsome , clever , rich , c...</td>\n",
       "      <td>austen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>emma</td>\n",
       "      <td>She youngest two daughters affectionate , ind...</td>\n",
       "      <td>austen</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Titles                                              Paras Authors\n",
       "0   emma                          [ Emma Jane Austen 1816 ]  austen\n",
       "1   emma                                           VOLUME I  austen\n",
       "2   emma                                          CHAPTER I  austen\n",
       "3   emma   Emma Woodhouse , handsome , clever , rich , c...  austen\n",
       "4   emma   She youngest two daughters affectionate , ind...  austen"
      ]
     },
     "execution_count": 291,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**=> after filtering the data is more cleaner**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 251
    },
    "colab_type": "code",
    "id": "lWLUqLCiVL7U",
    "outputId": "1b5ec7e8-90f0-409c-aba0-495b218c2983"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bible          24608\n",
       "austen          5265\n",
       "chesterton      4055\n",
       "edgeworth       3726\n",
       "melville        2793\n",
       "whitman         2478\n",
       "shakespeare     2372\n",
       "bryant          1194\n",
       "carroll          817\n",
       "blake            284\n",
       "burgess          266\n",
       "milton            29\n",
       "Name: Authors, dtype: int64"
      ]
     },
     "execution_count": 292,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Authors'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Total number of paras for each author => the data is imbalace (milton only 29 paras)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "split data to 20% test and 80% training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5dmfc-w2YvCF"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(data['Paras'], data['Authors'], test_size=0.2, random_state=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "wkirtx1cZOAN",
    "outputId": "9acb2be6-0371-4d2a-e8b3-37c083b482b8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training shape: (38309,)(38309,)\n",
      "testing shape : (9578,)(9578,)\n"
     ]
    }
   ],
   "source": [
    "print(\"training shape: {}{}\".format(X_train.shape,y_train.shape))\n",
    "print(\"testing shape : {}{}\".format(X_test.shape,y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dgUBphoqY-_o"
   },
   "source": [
    "## Bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "K0r1wtXOZA8A",
    "outputId": "28f56e80-6b0c-445f-f63e-1856cf6b9dc2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(38309, 5000)"
      ]
     },
     "execution_count": 295,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "count_vect = CountVectorizer(max_features = 5000)\n",
    "count_vect.fit(data['Paras'])\n",
    "X_train_counts = count_vect.transform(X_train)\n",
    "X_test_counts = count_vect.transform(X_test)\n",
    "X_train_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "x7edr5rZZpEV",
    "outputId": "6fc5aa82-9738-43fa-9d56-d3100417d334"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training shape: (38309, 5000)(38309,)\n",
      "testing shape : (9578, 5000)(9578,)\n"
     ]
    }
   ],
   "source": [
    "print(\"training shape: {}{}\".format(X_train_counts.shape,y_train.shape))\n",
    "print(\"testing shape : {}{}\".format(X_test_counts.shape,y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gKLbdQHSZ0uE"
   },
   "source": [
    "5000 Words in bag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 359
    },
    "colab_type": "code",
    "id": "V9ihhQcMZ2wU",
    "outputId": "3eac563d-0365-4f33-833e-faa9fbccdb2b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      austen       0.69      0.83      0.75      1003\n",
      "       bible       0.93      0.99      0.96      4939\n",
      "       blake       0.36      0.09      0.15        53\n",
      "      bryant       0.56      0.43      0.48       246\n",
      "     burgess       0.97      0.64      0.77        58\n",
      "     carroll       0.92      0.63      0.75       169\n",
      "  chesterton       0.73      0.73      0.73       803\n",
      "   edgeworth       0.71      0.57      0.63       758\n",
      "    melville       0.73      0.61      0.67       554\n",
      "      milton       0.50      0.67      0.57         3\n",
      " shakespeare       0.75      0.82      0.78       496\n",
      "     whitman       0.63      0.44      0.51       496\n",
      "\n",
      "    accuracy                           0.83      9578\n",
      "   macro avg       0.71      0.62      0.65      9578\n",
      "weighted avg       0.82      0.83      0.82      9578\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = RandomForestClassifier(n_estimators=20, random_state=1)\n",
    "model.fit(X_train_counts,y_train)\n",
    "pr = model.predict(X_test_counts)\n",
    "print(classification_report(y_test, pr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 359
    },
    "colab_type": "code",
    "id": "5_zx55puaHGg",
    "outputId": "9a4fba8a-14ae-4535-a871-26a6355c63e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      austen       0.66      0.74      0.69      1003\n",
      "       bible       0.94      0.94      0.94      4939\n",
      "       blake       0.24      0.15      0.18        53\n",
      "      bryant       0.38      0.45      0.41       246\n",
      "     burgess       0.84      0.66      0.74        58\n",
      "     carroll       0.83      0.66      0.73       169\n",
      "  chesterton       0.64      0.63      0.64       803\n",
      "   edgeworth       0.64      0.55      0.59       758\n",
      "    melville       0.60      0.58      0.59       554\n",
      "      milton       0.33      0.33      0.33         3\n",
      " shakespeare       0.60      0.73      0.66       496\n",
      "     whitman       0.48      0.43      0.45       496\n",
      "\n",
      "    accuracy                           0.78      9578\n",
      "   macro avg       0.60      0.57      0.58      9578\n",
      "weighted avg       0.78      0.78      0.78      9578\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = DecisionTreeClassifier(random_state=2)\n",
    "model.fit(X_train_counts,y_train)\n",
    "pr = model.predict(X_test_counts)\n",
    "print(classification_report(y_test, pr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 359
    },
    "colab_type": "code",
    "id": "PreExIqdcT_H",
    "outputId": "dc78db06-79bb-4002-d569-a921f0d4b8e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      austen       0.91      0.78      0.84      1003\n",
      "       bible       0.99      0.92      0.95      4939\n",
      "       blake       0.10      0.40      0.16        53\n",
      "      bryant       0.38      0.50      0.43       246\n",
      "     burgess       0.31      0.69      0.43        58\n",
      "     carroll       0.40      0.66      0.50       169\n",
      "  chesterton       0.81      0.64      0.72       803\n",
      "   edgeworth       0.68      0.78      0.72       758\n",
      "    melville       0.76      0.67      0.71       554\n",
      "      milton       0.07      0.67      0.12         3\n",
      " shakespeare       0.81      0.88      0.85       496\n",
      "     whitman       0.46      0.59      0.52       496\n",
      "\n",
      "    accuracy                           0.82      9578\n",
      "   macro avg       0.56      0.68      0.58      9578\n",
      "weighted avg       0.86      0.82      0.83      9578\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "model = GaussianNB()\n",
    "pr = model.fit(X_train_counts.toarray(), y_train)\n",
    "pr = model.predict(X_test_counts.toarray())\n",
    "print(classification_report(y_test, pr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 377
    },
    "colab_type": "code",
    "id": "Vm3nfwLLMfS4",
    "outputId": "7759bc5e-5ebe-4455-d941-54ae5c93ea02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83.10484266281128\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      austen       0.88      0.74      0.81      1003\n",
      "       bible       0.79      1.00      0.88      4939\n",
      "       blake       0.45      0.17      0.25        53\n",
      "      bryant       0.85      0.52      0.64       246\n",
      "     burgess       0.98      0.88      0.93        58\n",
      "     carroll       0.93      0.74      0.82       169\n",
      "  chesterton       0.91      0.67      0.77       803\n",
      "   edgeworth       0.95      0.64      0.77       758\n",
      "    melville       0.90      0.68      0.78       554\n",
      "      milton       0.40      0.67      0.50         3\n",
      " shakespeare       0.98      0.74      0.84       496\n",
      "     whitman       0.80      0.34      0.48       496\n",
      "\n",
      "    accuracy                           0.83      9578\n",
      "   macro avg       0.82      0.65      0.70      9578\n",
      "weighted avg       0.84      0.83      0.82      9578\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tick = time.time()\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "model = GradientBoostingClassifier(random_state=1)\n",
    "model.fit(X_train_counts,y_train)\n",
    "pr = model.predict(X_test_counts)\n",
    "print(time.time() - tick)\n",
    "print(classification_report(y_test, pr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**=>Gradient boosting and Randomforest give the best result over 80% of accuracy**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rKZg1sw9jhDV"
   },
   "source": [
    "some words like “the” will appear many times and their large counts will not be very meaningful in the encoded vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1Wb0ltkDkU-p"
   },
   "source": [
    "## TF-IDF\n",
    "\n",
    "An alternative is to calculate word frequencies, and by far the most popular method is called TF-IDF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cNEIqaYQly6z"
   },
   "source": [
    "Term Frequency: This summarizes how often a given word appears within a document.\n",
    "\n",
    "Inverse Document Frequency: This downscales words that appear a lot across documents.\n",
    "\n",
    "=> TF-IDF are word frequency scores that try to highlight words that are more interesting, e.g. frequent in a document but not across documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "CRxvEEPtkS37",
    "outputId": "d3226bf3-cc51-4c1a-9489-e798ff58b454"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(38309, 5000)"
      ]
     },
     "execution_count": 301,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# create the transform\n",
    "vectorizer = TfidfVectorizer(max_features= 5000)\n",
    "# tokenize and build vocab\n",
    "vectorizer.fit(data['Paras'])\n",
    "# summarize\n",
    "#print(vectorizer.vocabulary_)\n",
    "#print(vectorizer.idf_)\n",
    "# encode document\n",
    "X_train_counts = vectorizer.transform(X_train)\n",
    "X_test_counts = vectorizer.transform(X_test)\n",
    "# summarize encoded vector\n",
    "#print(vector.shape)\n",
    "#print(vector.toarray())\n",
    "X_train_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "63Ne-e3ppsIn",
    "outputId": "01cc829d-c84e-420a-f7cf-53377a9a4baa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training shape: (38309, 5000)(38309,)\n",
      "testing shape : (9578, 5000)(9578,)\n"
     ]
    }
   ],
   "source": [
    "print(\"training shape: {}{}\".format(X_train_counts.shape,y_train.shape))\n",
    "print(\"testing shape : {}{}\".format(X_test_counts.shape,y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 359
    },
    "colab_type": "code",
    "id": "eq90OS0IvnpA",
    "outputId": "d780e6d4-a472-4d2f-cf28-6aa277a79ce7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      austen       0.70      0.84      0.76      1003\n",
      "       bible       0.92      0.99      0.95      4939\n",
      "       blake       0.78      0.13      0.23        53\n",
      "      bryant       0.66      0.38      0.48       246\n",
      "     burgess       0.97      0.67      0.80        58\n",
      "     carroll       0.96      0.62      0.76       169\n",
      "  chesterton       0.74      0.72      0.73       803\n",
      "   edgeworth       0.73      0.58      0.65       758\n",
      "    melville       0.83      0.63      0.72       554\n",
      "      milton       0.50      0.67      0.57         3\n",
      " shakespeare       0.79      0.80      0.79       496\n",
      "     whitman       0.61      0.48      0.54       496\n",
      "\n",
      "    accuracy                           0.83      9578\n",
      "   macro avg       0.76      0.63      0.66      9578\n",
      "weighted avg       0.83      0.83      0.83      9578\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = RandomForestClassifier(n_estimators=20, random_state=1)\n",
    "model.fit(X_train_counts,y_train)\n",
    "pr = model.predict(X_test_counts)\n",
    "print(classification_report(y_test, pr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 359
    },
    "colab_type": "code",
    "id": "r1sKTP-SvsnV",
    "outputId": "f44b5c0e-07ee-4b78-da3b-34ee8aae318a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      austen       0.70      0.73      0.72      1003\n",
      "       bible       0.93      0.95      0.94      4939\n",
      "       blake       0.13      0.09      0.11        53\n",
      "      bryant       0.47      0.45      0.46       246\n",
      "     burgess       0.90      0.64      0.75        58\n",
      "     carroll       0.79      0.67      0.72       169\n",
      "  chesterton       0.64      0.64      0.64       803\n",
      "   edgeworth       0.63      0.59      0.61       758\n",
      "    melville       0.65      0.61      0.63       554\n",
      "      milton       0.25      0.33      0.29         3\n",
      " shakespeare       0.65      0.68      0.66       496\n",
      "     whitman       0.44      0.43      0.44       496\n",
      "\n",
      "    accuracy                           0.79      9578\n",
      "   macro avg       0.60      0.57      0.58      9578\n",
      "weighted avg       0.78      0.79      0.79      9578\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = DecisionTreeClassifier(random_state=2)\n",
    "model.fit(X_train_counts,y_train)\n",
    "pr = model.predict(X_test_counts)\n",
    "classification_report(y_test, pr)\n",
    "print(classification_report(y_test, pr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 359
    },
    "colab_type": "code",
    "id": "6QelInI9wJCM",
    "outputId": "8360dbd9-9ca5-4d70-95f2-768c6ece030b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      austen       0.86      0.81      0.83      1003\n",
      "       bible       0.99      0.92      0.96      4939\n",
      "       blake       0.17      0.38      0.23        53\n",
      "      bryant       0.35      0.48      0.41       246\n",
      "     burgess       0.19      0.67      0.29        58\n",
      "     carroll       0.41      0.63      0.49       169\n",
      "  chesterton       0.81      0.71      0.75       803\n",
      "   edgeworth       0.77      0.74      0.75       758\n",
      "    melville       0.69      0.72      0.70       554\n",
      "      milton       0.05      0.67      0.10         3\n",
      " shakespeare       0.86      0.86      0.86       496\n",
      "     whitman       0.52      0.56      0.54       496\n",
      "\n",
      "    accuracy                           0.83      9578\n",
      "   macro avg       0.55      0.68      0.58      9578\n",
      "weighted avg       0.86      0.83      0.84      9578\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "model = GaussianNB()\n",
    "pr = model.fit(X_train_counts.toarray(), y_train)\n",
    "pr = model.predict(X_test_counts.toarray())\n",
    "print(classification_report(y_test, pr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 377
    },
    "colab_type": "code",
    "id": "jvhuq0Y7w28h",
    "outputId": "301516e9-609e-4236-830b-c381e92bb374"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "259.0457031726837\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      austen       0.87      0.77      0.82      1003\n",
      "       bible       0.79      1.00      0.88      4939\n",
      "       blake       0.27      0.11      0.16        53\n",
      "      bryant       0.81      0.52      0.64       246\n",
      "     burgess       1.00      0.79      0.88        58\n",
      "     carroll       0.88      0.73      0.80       169\n",
      "  chesterton       0.91      0.67      0.77       803\n",
      "   edgeworth       0.94      0.65      0.77       758\n",
      "    melville       0.92      0.67      0.77       554\n",
      "      milton       0.33      0.33      0.33         3\n",
      " shakespeare       0.98      0.73      0.84       496\n",
      "     whitman       0.82      0.34      0.49       496\n",
      "\n",
      "    accuracy                           0.83      9578\n",
      "   macro avg       0.79      0.61      0.68      9578\n",
      "weighted avg       0.84      0.83      0.82      9578\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tick = time.time()\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "model = GradientBoostingClassifier(random_state=1)\n",
    "model.fit(X_train_counts,y_train)\n",
    "pr = model.predict(X_test_counts)\n",
    "print(time.time() - tick)\n",
    "print(classification_report(y_test, pr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**=>The accuracy of the best is still 83%, but accuaracy of Naive Bayes and Decision tree are improved when comparing to Bagofword**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xVmVyR_Gx7Tf"
   },
   "source": [
    "## Word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JKzSL1_Kx22p"
   },
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "from string import punctuation\n",
    "punc = set(punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 233
    },
    "colab_type": "code",
    "id": "Qfrzd7JZbzug",
    "outputId": "827f405d-d1d1-43a5-f331-4e2ab56cdd9f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                     [[, Emma, by, Jane, Austen, 1816, ]]\n",
       "1                                              [VOLUME, I]\n",
       "2                                             [CHAPTER, I]\n",
       "3        [Emma, Woodhouse, ,, handsome, ,, clever, ,, a...\n",
       "4        [She, was, the, youngest, of, the, two, daught...\n",
       "                               ...                        \n",
       "47882                      [}, Good, -, Bye, My, Fancy, !]\n",
       "47883    [Good, -, bye, my, Fancy, !, Farewell, dear, m...\n",
       "47884    [Now, for, my, last, --, let, me, look, back, ...\n",
       "47885    [Long, have, we, lived, ,, joy, ', d, ,, cares...\n",
       "47886    [Yet, let, me, not, be, too, hasty, ,, Long, i...\n",
       "Name: Paras, Length: 47887, dtype: object"
      ]
     },
     "execution_count": 308,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataWV = dataOrig['Paras'].copy()\n",
    "dataWV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 233
    },
    "colab_type": "code",
    "id": "4IlGiMtcyH-q",
    "outputId": "44777ee3-fc02-4ab2-edef-891c1c53b564"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                               [Emma, Jane, Austen, 1816]\n",
       "1                                              [VOLUME, I]\n",
       "2                                             [CHAPTER, I]\n",
       "3        [Emma, Woodhouse, handsome, clever, rich, comf...\n",
       "4        [She, youngest, two, daughters, affectionate, ...\n",
       "                               ...                        \n",
       "47882                               [Good, Bye, My, Fancy]\n",
       "47883    [Good, bye, Fancy, Farewell, dear, mate, dear,...\n",
       "47884    [Now, last, --, let, look, back, moment, The, ...\n",
       "47885    [Long, lived, joy, caress, together, Delightfu...\n",
       "47886    [Yet, let, hasty, Long, indeed, lived, slept, ...\n",
       "Name: Paras, Length: 47887, dtype: object"
      ]
     },
     "execution_count": 309,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english')) \n",
    "for i in range(len(dataWV)):\n",
    "  words = []\n",
    "  for w in dataWV[i]:\n",
    "    if not( ( w in stop_words) or (w in punc )) :\n",
    "      words.append(w) \n",
    "  dataWV[i] = words\n",
    "dataWV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EdzRILIlCdoA"
   },
   "outputs": [],
   "source": [
    "sz = 200\n",
    "model = Word2Vec(dataWV, size=sz, window=5, min_count=1, workers=4, iter=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "T4EH7hktCe8V"
   },
   "outputs": [],
   "source": [
    "#model.wv.most_similar(positive=\"girl\", topn =3)\n",
    "#len(model.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qIFaA-ZUGwLm"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "data_train, data_test, y_train, y_test = train_test_split(dataWV, dataOrig['Authors'], test_size=0.2, random_state=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5Z5a6rRojFNe"
   },
   "outputs": [],
   "source": [
    "import numpy as np  # Make sure that numpy is imported\n",
    "\n",
    "def makeFeatureVec(words, model, num_features):\n",
    "    # Function to average all of the word vectors in a given\n",
    "    # paragraph\n",
    "    #\n",
    "    # Pre-initialize an empty numpy array (for speed)\n",
    "    featureVec = np.zeros((num_features,),dtype=\"float32\")\n",
    "    #\n",
    "    nwords = 0.\n",
    "    # \n",
    "    # Index2word is a list that contains the names of the words in \n",
    "    # the model's vocabulary. Convert it to a set, for speed \n",
    "    index2word_set = set(model.wv.index2word)\n",
    "    #\n",
    "    # Loop over each word in the review and, if it is in the model's\n",
    "    # vocaublary, add its feature vector to the total\n",
    "    for word in words:\n",
    "        if word in index2word_set: \n",
    "            nwords = nwords + 1.\n",
    "            featureVec = np.add(featureVec,model[word])\n",
    "    # \n",
    "    # Divide the result by the number of words to get the average\n",
    "    featureVec = np.divide(featureVec,nwords)\n",
    "    return featureVec\n",
    "\n",
    "\n",
    "def getAvgFeatureVecs(reviews, model, num_features):\n",
    "    # Given a set of reviews (each one a list of words), calculate \n",
    "    # the average feature vector for each one and return a 2D numpy array \n",
    "    # \n",
    "    # Initialize a counter\n",
    "    counter = 0.\n",
    "    # \n",
    "    # Preallocate a 2D numpy array, for speed\n",
    "    reviewFeatureVecs = np.zeros((len(reviews),num_features),dtype=\"float32\")\n",
    "    # \n",
    "    # Loop through the reviews\n",
    "    for review in reviews:\n",
    "       #\n",
    "       # Print a status message every 1000th review\n",
    "       if counter%10000. == 0.:\n",
    "           print (\"Review {} of {}\" .format(counter, len(reviews)))\n",
    "\n",
    "       # Call the function (defined above) that makes average feature vectors\n",
    "       reviewFeatureVecs[int(counter)] = makeFeatureVec(review, model, num_features)\n",
    "       #\n",
    "       # Increment the counter\n",
    "       counter = counter + 1.\n",
    "    return reviewFeatureVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 125
    },
    "colab_type": "code",
    "id": "0lcApcpAl8m4",
    "outputId": "b12ab038-02c6-4ab4-b982-2fc86dd83b16"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 0.0 of 38309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:24: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 10000.0 of 38309\n",
      "Review 20000.0 of 38309\n",
      "Review 30000.0 of 38309\n",
      "Review 0.0 of 9578\n"
     ]
    }
   ],
   "source": [
    "X_train = getAvgFeatureVecs(data_train, model, sz)\n",
    "X_test = getAvgFeatureVecs(data_test, model, sz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RtesbmyhsUwL"
   },
   "outputs": [],
   "source": [
    "X_train = np.nan_to_num(X_train) \n",
    "X_test = np.nan_to_num(X_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 377
    },
    "colab_type": "code",
    "id": "GrYAk5yuntzu",
    "outputId": "cda7c6bb-5434-4587-d1d2-a4a7b6ef28d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      austen       0.67      0.79      0.72      1003\n",
      "       bible       0.98      1.00      0.99      4939\n",
      "       blake       0.41      0.17      0.24        53\n",
      "      bryant       0.47      0.21      0.29       246\n",
      "     burgess       0.50      0.09      0.15        58\n",
      "     carroll       0.64      0.28      0.39       169\n",
      "  chesterton       0.50      0.61      0.55       803\n",
      "   edgeworth       0.41      0.37      0.39       758\n",
      "    melville       0.57      0.48      0.52       554\n",
      "      milton       1.00      0.67      0.80         3\n",
      " shakespeare       0.76      0.79      0.77       496\n",
      "     whitman       0.61      0.64      0.62       496\n",
      "\n",
      "    accuracy                           0.79      9578\n",
      "   macro avg       0.63      0.51      0.54      9578\n",
      "weighted avg       0.78      0.79      0.78      9578\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "17.42493462562561"
      ]
     },
     "execution_count": 316,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tick = time.time()\n",
    "model = RandomForestClassifier(n_estimators=20, random_state=1)\n",
    "model.fit(X_train,y_train)\n",
    "pr = model.predict(X_test)\n",
    "print(classification_report(y_test, pr))\n",
    "time.time() - tick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 359
    },
    "colab_type": "code",
    "id": "hsnRDKOGsNgo",
    "outputId": "e40ceafc-ce54-433b-d722-76d9c0813e34"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      austen       0.64      0.64      0.64      1003\n",
      "       bible       0.98      0.98      0.98      4939\n",
      "       blake       0.12      0.13      0.13        53\n",
      "      bryant       0.19      0.21      0.20       246\n",
      "     burgess       0.14      0.16      0.15        58\n",
      "     carroll       0.25      0.26      0.26       169\n",
      "  chesterton       0.40      0.38      0.39       803\n",
      "   edgeworth       0.30      0.31      0.31       758\n",
      "    melville       0.40      0.38      0.39       554\n",
      "      milton       0.33      1.00      0.50         3\n",
      " shakespeare       0.67      0.66      0.66       496\n",
      "     whitman       0.51      0.52      0.52       496\n",
      "\n",
      "    accuracy                           0.72      9578\n",
      "   macro avg       0.41      0.47      0.43      9578\n",
      "weighted avg       0.73      0.72      0.72      9578\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = DecisionTreeClassifier(random_state=2)\n",
    "model.fit(X_train,y_train)\n",
    "pr = model.predict(X_test)\n",
    "classification_report(y_test, pr)\n",
    "print(classification_report(y_test, pr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 359
    },
    "colab_type": "code",
    "id": "rUACfDmpsw4Z",
    "outputId": "48945441-56d5-452d-a297-c83658207d17"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      austen       0.60      0.68      0.64      1003\n",
      "       bible       0.98      0.92      0.95      4939\n",
      "       blake       0.10      0.38      0.16        53\n",
      "      bryant       0.18      0.20      0.19       246\n",
      "     burgess       0.08      0.38      0.13        58\n",
      "     carroll       0.28      0.44      0.34       169\n",
      "  chesterton       0.48      0.13      0.21       803\n",
      "   edgeworth       0.32      0.22      0.26       758\n",
      "    melville       0.21      0.13      0.16       554\n",
      "      milton       0.01      1.00      0.02         3\n",
      " shakespeare       0.58      0.45      0.51       496\n",
      "     whitman       0.26      0.50      0.34       496\n",
      "\n",
      "    accuracy                           0.65      9578\n",
      "   macro avg       0.34      0.45      0.33      9578\n",
      "weighted avg       0.70      0.65      0.66      9578\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "model = GaussianNB()\n",
    "pr = model.fit(X_train, y_train)\n",
    "pr = model.predict(X_test)\n",
    "print(classification_report(y_test, pr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 377
    },
    "colab_type": "code",
    "id": "CmnwH5Jst3on",
    "outputId": "7db78882-0c63-446a-b1a1-e1f3b851d339"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "972.6748280525208\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      austen       0.71      0.79      0.75      1003\n",
      "       bible       0.99      0.99      0.99      4939\n",
      "       blake       0.35      0.21      0.26        53\n",
      "      bryant       0.49      0.25      0.33       246\n",
      "     burgess       0.52      0.19      0.28        58\n",
      "     carroll       0.56      0.38      0.45       169\n",
      "  chesterton       0.55      0.61      0.58       803\n",
      "   edgeworth       0.45      0.42      0.43       758\n",
      "    melville       0.59      0.53      0.56       554\n",
      "      milton       0.00      0.00      0.00         3\n",
      " shakespeare       0.74      0.79      0.76       496\n",
      "     whitman       0.60      0.70      0.65       496\n",
      "\n",
      "    accuracy                           0.80      9578\n",
      "   macro avg       0.55      0.49      0.50      9578\n",
      "weighted avg       0.80      0.80      0.80      9578\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tick = time.time()\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "model = GradientBoostingClassifier(random_state=1)\n",
    "model.fit(X_train,y_train)\n",
    "pr = model.predict(X_test)\n",
    "print(time.time() - tick)\n",
    "print(classification_report(y_test, pr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S2wO__6SveAJ"
   },
   "source": [
    "**The performance worse than the others NLP techniques, the best case is 80% while the worse one is 65%**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oZzYfOTcw2p2"
   },
   "source": [
    "## LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "flxFIOWp4EAn"
   },
   "outputs": [],
   "source": [
    "# Importing Gensim\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "\n",
    "# Creating the term dictionary of our courpus, where every unique term is assigned an index. \n",
    "dictionary = corpora.Dictionary(dataWV)\n",
    "\n",
    "# Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above.\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in dataWV]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "PzbEDRrs4XEX",
    "outputId": "a28a2bb6-4d39-4f08-9bb1-5b5d68a12cc2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50957"
      ]
     },
     "execution_count": 322,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "Bmxw-SyCxGcY",
    "outputId": "4a251cd0-bb36-4b6b-b0fd-ca37e0614cec"
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-323-059b5b7c12f3>\"\u001b[0;36m, line \u001b[0;32m6\u001b[0m\n\u001b[0;31m    ldamodel = Lda(doc_term_matrix, num_topics=7, id2word = dictionary, passes=50)Saliency\u001b[0m\n\u001b[0m                                                                                         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "tick = time.time()\n",
    "# Creating the object for LDA model using gensim library\n",
    "Lda = gensim.models.ldamodel.LdaModel\n",
    "\n",
    "# Running and Training LDA model on the document term matrix.\n",
    "ldamodel = Lda(doc_term_matrix, num_topics=7, id2word = dictionary, passes=50)Saliency\n",
    "time.time - tick"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Still not implemnet LDA to show the top 10 words**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**=>BagofWords: Gradient boosting and Randomforest give the best result over 80% of accuracy**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**=>TF-IDF: The accuracy of the best is still 83%, but accuaracy of Naive Bayes and Decision tree are improved when comparing to Bagofword**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**=>Word2Vec: The performance worse than the others NLP techniques, the best case is 80% while the worse one is 65%**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Still not implemnet LDA to show the top 10 words**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**=> Wait for next week to reference project of other peoples in class to improve and revise my project**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Author.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
