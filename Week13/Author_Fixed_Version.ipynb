{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PvEs_-FMKukG"
   },
   "source": [
    "\n",
    "# <center> Author Classification </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ewQAVhTrKukK"
   },
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "11hzLcESKukM"
   },
   "source": [
    "Using NLP and techniques to classify author from texts from Gutenberg project.\n",
    "1. Pre-process data using Spacy and other methods.\n",
    "2. Perform data exploration\n",
    "3. Using Bag of Word, apply supervised models such as Naive Bayes,  Decision Tree, Random Forest, and Gradient Boosting.\n",
    "4. Similar to 3., but using TF-IDF.\n",
    "5. Similar to 3., but using word2vec.\n",
    "6. Using unsupervised technique for clustering authors. <font color='red'>**(ADDED)**</font>\n",
    "7. Using LSA and LDA, print out top ten words (with their highest loading) for each topic modeling.<font color='red'>**(ADDED)**</font>\n",
    "\n",
    "****\n",
    "<font color= 'red'> **Fixed Version**\n",
    "\n",
    "**=> Lower all words and remove author in each samples**\n",
    "\n",
    "\n",
    "**=> Tuning all model when using BoW technique to imporve performance**\n",
    "\n",
    "**=> Apply the tuned hyperparameters in BoW to TF-IDF, Word2Vec to imrpove the performance**\n",
    "\n",
    "**=> Tuning hyperparmeters in Word2Vec techniques to gain higher accuaracy**\n",
    "\n",
    "**=> Clustering using Word2Vec**\n",
    "\n",
    "**=> Apply LSA and LDA for topic modeling**\n",
    "    \n",
    "\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f-bnWFFqKukO",
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Introduction\" data-toc-modified-id=\"Introduction-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Introduction</a></span></li><li><span><a href=\"#Explore-Data\" data-toc-modified-id=\"Explore-Data-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Explore Data</a></span></li><li><span><a href=\"#Prepare-Data\" data-toc-modified-id=\"Prepare-Data-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Prepare Data</a></span></li><li><span><a href=\"#Bag-of-words\" data-toc-modified-id=\"Bag-of-words-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Bag of words</a></span></li><li><span><a href=\"#TF-IDF\" data-toc-modified-id=\"TF-IDF-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>TF-IDF</a></span></li><li><span><a href=\"#Word2vec\" data-toc-modified-id=\"Word2vec-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Word2vec</a></span></li><li><span><a href=\"#Clustering\" data-toc-modified-id=\"Clustering-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Clustering</a></span></li><li><span><a href=\"#LSA\" data-toc-modified-id=\"LSA-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>LSA</a></span></li><li><span><a href=\"#LDA\" data-toc-modified-id=\"LDA-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>LDA</a></span></li><li><span><a href=\"#Conclusion\" data-toc-modified-id=\"Conclusion-10\"><span class=\"toc-item-num\">10&nbsp;&nbsp;</span>Conclusion</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hYMUBNb_KukQ"
   },
   "source": [
    "## Explore Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "colab_type": "code",
    "id": "ME2U1pNSNdhU",
    "outputId": "735d04fe-2d28-4ae9-85fc-7663e5d1ff84"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/gutenberg.zip.\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import gutenberg\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from itertools import chain\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "nltk.download('gutenberg')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P5YjuGJEmc__"
   },
   "source": [
    "https://machinelearningmastery.com/prepare-text-data-machine-learning-scikit-learn/\n",
    "\n",
    "https://www.kaggle.com/c/word2vec-nlp-tutorial/overview/part-3-more-fun-with-word-vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 341
    },
    "colab_type": "code",
    "id": "-ibYL6h7N81Q",
    "outputId": "4ed9c900-90c5-4522-e127-1c8bc69a381b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['austen-emma.txt',\n",
       " 'austen-persuasion.txt',\n",
       " 'austen-sense.txt',\n",
       " 'bible-kjv.txt',\n",
       " 'blake-poems.txt',\n",
       " 'bryant-stories.txt',\n",
       " 'burgess-busterbrown.txt',\n",
       " 'carroll-alice.txt',\n",
       " 'chesterton-ball.txt',\n",
       " 'chesterton-brown.txt',\n",
       " 'chesterton-thursday.txt',\n",
       " 'edgeworth-parents.txt',\n",
       " 'melville-moby_dick.txt',\n",
       " 'milton-paradise.txt',\n",
       " 'shakespeare-caesar.txt',\n",
       " 'shakespeare-hamlet.txt',\n",
       " 'shakespeare-macbeth.txt',\n",
       " 'whitman-leaves.txt']"
      ]
     },
     "execution_count": 2,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Novels = gutenberg.fileids()\n",
    "Novels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sRM03vkYKuki"
   },
   "source": [
    "The data is name of author followed title of the book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "75XdoSG-OFha"
   },
   "outputs": [],
   "source": [
    "numNovels = len(gutenberg.fileids())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sy4z_95IKuko"
   },
   "source": [
    "There are 18 book in this project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 251
    },
    "colab_type": "code",
    "id": "cUwjMei6PR5j",
    "outputId": "55dd865d-80d7-4d5f-d459-f14f31061ad7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['austen',\n",
       " 'bible',\n",
       " 'blake',\n",
       " 'bryant',\n",
       " 'burgess',\n",
       " 'carroll',\n",
       " 'chesterton',\n",
       " 'edgeworth',\n",
       " 'melville',\n",
       " 'milton',\n",
       " 'shakespeare',\n",
       " 'whitman']"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Authors = []\n",
    "for i in range(numNovels):\n",
    "  author = Novels[i].split('-')[0]\n",
    "  if  (author in Authors ):\n",
    "    continue\n",
    "  Authors.append(Novels[i].split('-')[0])\n",
    "print(len(Authors))\n",
    "Authors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NothWDX2Kukt"
   },
   "source": [
    "There are 12 authors who wrote 18 books above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 341
    },
    "colab_type": "code",
    "id": "EX7WH-vxPVCy",
    "outputId": "a367bb7a-862f-4102-9b95-27ae78703546"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "austen-emma has 192427 words\n",
      "austen-persuasion has 98171 words\n",
      "austen-sense has 141576 words\n",
      "bible-kjv has 1010654 words\n",
      "blake-poems has 8354 words\n",
      "bryant-stories has 55563 words\n",
      "burgess-busterbrown has 18963 words\n",
      "carroll-alice has 34110 words\n",
      "chesterton-ball has 96996 words\n",
      "chesterton-brown has 86063 words\n",
      "chesterton-thursday has 69213 words\n",
      "edgeworth-parents has 210663 words\n",
      "melville-moby_dick has 260819 words\n",
      "milton-paradise has 96825 words\n",
      "shakespeare-caesar has 25833 words\n",
      "shakespeare-hamlet has 37360 words\n",
      "shakespeare-macbeth has 23140 words\n",
      "whitman-leaves has 154883 words\n"
     ]
    }
   ],
   "source": [
    "for i in Novels:\n",
    "  print(i.split('.')[0] + \" has \" + str(len(gutenberg.words(i))) + ' words'  )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v-W1Pr5-Kukz"
   },
   "source": [
    "Results above show total of words in each book."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lEFBwXALKuk0"
   },
   "source": [
    "They will be transformed to dataframe for easier to read, and this data frame sumarize all information about words, senteces and vocalbulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AEoKiS_KYjaF"
   },
   "outputs": [],
   "source": [
    "num_word = []\n",
    "num_sent = []\n",
    "num_vocab = []\n",
    "for fileid in gutenberg.fileids():\n",
    "    num_word.append(len(gutenberg.words(fileid)) )\n",
    "    num_sent.append(len(gutenberg.sents(fileid)) )\n",
    "    num_vocab.append(len(set(gutenberg.words(fileid))) )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JQlwE_l4fo6m"
   },
   "outputs": [],
   "source": [
    "suma = pd.DataFrame( index= Novels, columns = ['Words','Sentences','Vocabulary'], data = np.array([num_word, num_sent,num_vocab]).T )  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 607
    },
    "colab_type": "code",
    "id": "-_FfuEntjFay",
    "outputId": "5a502e62-b39d-4f6a-ec19-53db04cc7e99"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Words</th>\n",
       "      <th>Sentences</th>\n",
       "      <th>Vocabulary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>austen-emma.txt</th>\n",
       "      <td>192427</td>\n",
       "      <td>7752</td>\n",
       "      <td>7811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>austen-persuasion.txt</th>\n",
       "      <td>98171</td>\n",
       "      <td>3747</td>\n",
       "      <td>6132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>austen-sense.txt</th>\n",
       "      <td>141576</td>\n",
       "      <td>4999</td>\n",
       "      <td>6833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bible-kjv.txt</th>\n",
       "      <td>1010654</td>\n",
       "      <td>30103</td>\n",
       "      <td>13769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>blake-poems.txt</th>\n",
       "      <td>8354</td>\n",
       "      <td>438</td>\n",
       "      <td>1820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bryant-stories.txt</th>\n",
       "      <td>55563</td>\n",
       "      <td>2863</td>\n",
       "      <td>4420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>burgess-busterbrown.txt</th>\n",
       "      <td>18963</td>\n",
       "      <td>1054</td>\n",
       "      <td>1764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>carroll-alice.txt</th>\n",
       "      <td>34110</td>\n",
       "      <td>1703</td>\n",
       "      <td>3016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chesterton-ball.txt</th>\n",
       "      <td>96996</td>\n",
       "      <td>4779</td>\n",
       "      <td>8947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chesterton-brown.txt</th>\n",
       "      <td>86063</td>\n",
       "      <td>3806</td>\n",
       "      <td>8299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chesterton-thursday.txt</th>\n",
       "      <td>69213</td>\n",
       "      <td>3742</td>\n",
       "      <td>6807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>edgeworth-parents.txt</th>\n",
       "      <td>210663</td>\n",
       "      <td>10230</td>\n",
       "      <td>9593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>melville-moby_dick.txt</th>\n",
       "      <td>260819</td>\n",
       "      <td>10059</td>\n",
       "      <td>19317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>milton-paradise.txt</th>\n",
       "      <td>96825</td>\n",
       "      <td>1851</td>\n",
       "      <td>10751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>shakespeare-caesar.txt</th>\n",
       "      <td>25833</td>\n",
       "      <td>2163</td>\n",
       "      <td>3560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>shakespeare-hamlet.txt</th>\n",
       "      <td>37360</td>\n",
       "      <td>3106</td>\n",
       "      <td>5447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>shakespeare-macbeth.txt</th>\n",
       "      <td>23140</td>\n",
       "      <td>1907</td>\n",
       "      <td>4017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>whitman-leaves.txt</th>\n",
       "      <td>154883</td>\n",
       "      <td>4250</td>\n",
       "      <td>14329</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Words  Sentences  Vocabulary\n",
       "austen-emma.txt           192427       7752        7811\n",
       "austen-persuasion.txt      98171       3747        6132\n",
       "austen-sense.txt          141576       4999        6833\n",
       "bible-kjv.txt            1010654      30103       13769\n",
       "blake-poems.txt             8354        438        1820\n",
       "bryant-stories.txt         55563       2863        4420\n",
       "burgess-busterbrown.txt    18963       1054        1764\n",
       "carroll-alice.txt          34110       1703        3016\n",
       "chesterton-ball.txt        96996       4779        8947\n",
       "chesterton-brown.txt       86063       3806        8299\n",
       "chesterton-thursday.txt    69213       3742        6807\n",
       "edgeworth-parents.txt     210663      10230        9593\n",
       "melville-moby_dick.txt    260819      10059       19317\n",
       "milton-paradise.txt        96825       1851       10751\n",
       "shakespeare-caesar.txt     25833       2163        3560\n",
       "shakespeare-hamlet.txt     37360       3106        5447\n",
       "shakespeare-macbeth.txt    23140       1907        4017\n",
       "whitman-leaves.txt        154883       4250       14329"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "suma\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sbc2bslkkAGa"
   },
   "source": [
    "**bible-kjv is the book which has largest amount of words than the others. while blake-poems is the least. It can understand that poems is less words than novels.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Won58ctRKulH"
   },
   "source": [
    "Now extract an random book to show its content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eRyc9U7AKulI"
   },
   "source": [
    "**=> it is raw data beccasue it has a lot symbol like \\n, ...**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "uoJqkJgyj_8L",
    "outputId": "963be1a3-518b-46f0-fada-1ff8b65ba1a2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['[', 'Emma', 'by', 'Jane', 'Austen', '1816', ']']], [['VOLUME', 'I']]]"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gutenberg.paras('austen-emma.txt')[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Nmg4ShfKKulP"
   },
   "source": [
    "**=>Because the number of sentences are too large, this project focuse on \"paras\" which consider as set of sentences. to reduce the number of samples**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 359
    },
    "colab_type": "code",
    "id": "sOUtQ7fLH77x",
    "outputId": "70a22fe2-2f91-4390-b49b-05ef7d6cae2c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "austen-emma has 2371 paragraphs\n",
      "austen-persuasion has 1032 paragraphs\n",
      "austen-sense has 1862 paragraphs\n",
      "bible-kjv has 24608 paragraphs\n",
      "blake-poems has 284 paragraphs\n",
      "bryant-stories has 1194 paragraphs\n",
      "burgess-busterbrown has 266 paragraphs\n",
      "carroll-alice has 817 paragraphs\n",
      "chesterton-ball has 1606 paragraphs\n",
      "chesterton-brown has 1161 paragraphs\n",
      "chesterton-thursday has 1288 paragraphs\n",
      "edgeworth-parents has 3726 paragraphs\n",
      "melville-moby_dick has 2793 paragraphs\n",
      "milton-paradise has 29 paragraphs\n",
      "shakespeare-caesar has 744 paragraphs\n",
      "shakespeare-hamlet has 950 paragraphs\n",
      "shakespeare-macbeth has 678 paragraphs\n",
      "whitman-leaves has 2478 paragraphs\n",
      "47887\n"
     ]
    }
   ],
   "source": [
    "s = 0\n",
    "for i in Novels:\n",
    "  print(i.split('.')[0] + \" has \" +  str(len(gutenberg.paras(i)))  + \" paragraphs\")\n",
    "  s = s + len(gutenberg.paras(i))\n",
    "print(s)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xDnqRNKpKulX"
   },
   "source": [
    "**Each sample will have 500 paras to reduce the number of samples and process data faster**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "O3vQtFl-ufwh",
    "outputId": "a3aedcaa-6748-46c6-a7a2-575db611e691"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blake-poems has 284 paragraphs\n",
      "burgess-busterbrown has 266 paragraphs\n",
      "milton-paradise has 29 paragraphs\n"
     ]
    }
   ],
   "source": [
    "for i in Novels:\n",
    "  if (len(gutenberg.paras(i)) < 500):\n",
    "    print(i.split('.')[0] + \" has \" +  str(len(gutenberg.paras(i)))  + \" paragraphs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2iTGzix0EAiP"
   },
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Rm2v6BG8HFNj"
   },
   "source": [
    "Generate data from the books which has 3 features titles, paras and authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "JdDOE9nrwo_K",
    "outputId": "e7f921d8-a7de-4bfe-e13c-645972248abd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.439829587936401\n"
     ]
    }
   ],
   "source": [
    "# Titles, Sentences, Authors\n",
    "Titles = []\n",
    "Paras = []\n",
    "AuthorsLS = []\n",
    "import time\n",
    "tick = time.time()\n",
    "# get the data\n",
    "from itertools import chain\n",
    "\n",
    "for fileid in gutenberg.fileids():\n",
    "    author = fileid.split('-')[0] \n",
    "    kk = gutenberg.paras(fileid) \n",
    "    title = fileid.split('-')[1].split('.')[0] \n",
    "    for para in kk:\n",
    "        AuthorsLS.append(author)\n",
    "        Titles.append(title)\n",
    "        para = list(chain.from_iterable(para)) \n",
    "        Paras.append(para)\n",
    "    \n",
    "print(time.time() - tick)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "colab_type": "code",
    "id": "K3IwO7JqV404",
    "outputId": "d5e39170-8e69-48de-ce75-85104b247fa2"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Titles</th>\n",
       "      <th>Paras</th>\n",
       "      <th>Authors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>emma</td>\n",
       "      <td>[[, Emma, by, Jane, Austen, 1816, ]]</td>\n",
       "      <td>austen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>emma</td>\n",
       "      <td>[VOLUME, I]</td>\n",
       "      <td>austen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>emma</td>\n",
       "      <td>[CHAPTER, I]</td>\n",
       "      <td>austen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>emma</td>\n",
       "      <td>[Emma, Woodhouse, ,, handsome, ,, clever, ,, a...</td>\n",
       "      <td>austen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>emma</td>\n",
       "      <td>[She, was, the, youngest, of, the, two, daught...</td>\n",
       "      <td>austen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47882</th>\n",
       "      <td>leaves</td>\n",
       "      <td>[}, Good, -, Bye, My, Fancy, !]</td>\n",
       "      <td>whitman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47883</th>\n",
       "      <td>leaves</td>\n",
       "      <td>[Good, -, bye, my, Fancy, !, Farewell, dear, m...</td>\n",
       "      <td>whitman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47884</th>\n",
       "      <td>leaves</td>\n",
       "      <td>[Now, for, my, last, --, let, me, look, back, ...</td>\n",
       "      <td>whitman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47885</th>\n",
       "      <td>leaves</td>\n",
       "      <td>[Long, have, we, lived, ,, joy, ', d, ,, cares...</td>\n",
       "      <td>whitman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47886</th>\n",
       "      <td>leaves</td>\n",
       "      <td>[Yet, let, me, not, be, too, hasty, ,, Long, i...</td>\n",
       "      <td>whitman</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>47887 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Titles                                              Paras  Authors\n",
       "0        emma               [[, Emma, by, Jane, Austen, 1816, ]]   austen\n",
       "1        emma                                        [VOLUME, I]   austen\n",
       "2        emma                                       [CHAPTER, I]   austen\n",
       "3        emma  [Emma, Woodhouse, ,, handsome, ,, clever, ,, a...   austen\n",
       "4        emma  [She, was, the, youngest, of, the, two, daught...   austen\n",
       "...       ...                                                ...      ...\n",
       "47882  leaves                    [}, Good, -, Bye, My, Fancy, !]  whitman\n",
       "47883  leaves  [Good, -, bye, my, Fancy, !, Farewell, dear, m...  whitman\n",
       "47884  leaves  [Now, for, my, last, --, let, me, look, back, ...  whitman\n",
       "47885  leaves  [Long, have, we, lived, ,, joy, ', d, ,, cares...  whitman\n",
       "47886  leaves  [Yet, let, me, not, be, too, hasty, ,, Long, i...  whitman\n",
       "\n",
       "[47887 rows x 3 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataOrig = pd.DataFrame({ 'Titles' : Titles,\n",
    "                      'Paras':    Paras,\n",
    "                      'Authors': AuthorsLS})\n",
    "dataOrig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6KKOWWFBKulq"
   },
   "source": [
    "Using stop word in english to filter data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color= 'red'>**=> lower all char and remove author in each samples**</cetner>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mcwYd-JkWFUh"
   },
   "outputs": [],
   "source": [
    "import string\n",
    "data = dataOrig.copy()\n",
    "stop_words = set(stopwords.words('english') + Authors + list(string.punctuation))\n",
    "for i in range(data.shape[0]):\n",
    "  words = ''\n",
    "  for w in data[\"Paras\"][i]:\n",
    "    if not w.lower() in stop_words:\n",
    "        words = words + \" \" + w.lower() \n",
    "  data[\"Paras\"][i] = words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "kPJFl9CUPjbP",
    "outputId": "bd74d091-de9c-40ae-cac6-1af0559c30ac"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Titles</th>\n",
       "      <th>Paras</th>\n",
       "      <th>Authors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>emma</td>\n",
       "      <td>emma jane 1816</td>\n",
       "      <td>austen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>emma</td>\n",
       "      <td>volume</td>\n",
       "      <td>austen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>emma</td>\n",
       "      <td>chapter</td>\n",
       "      <td>austen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>emma</td>\n",
       "      <td>emma woodhouse handsome clever rich comfortab...</td>\n",
       "      <td>austen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>emma</td>\n",
       "      <td>youngest two daughters affectionate indulgent...</td>\n",
       "      <td>austen</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Titles                                              Paras Authors\n",
       "0   emma                                     emma jane 1816  austen\n",
       "1   emma                                             volume  austen\n",
       "2   emma                                            chapter  austen\n",
       "3   emma   emma woodhouse handsome clever rich comfortab...  austen\n",
       "4   emma   youngest two daughters affectionate indulgent...  austen"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lHMT3bkuKul3"
   },
   "source": [
    "**=> after filtering the data is more cleaner**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 251
    },
    "colab_type": "code",
    "id": "lWLUqLCiVL7U",
    "outputId": "6774aa7b-c56c-46ac-b466-f800bfab4bfa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bible          24608\n",
       "austen          5265\n",
       "chesterton      4055\n",
       "edgeworth       3726\n",
       "melville        2793\n",
       "whitman         2478\n",
       "shakespeare     2372\n",
       "bryant          1194\n",
       "carroll          817\n",
       "blake            284\n",
       "burgess          266\n",
       "milton            29\n",
       "Name: Authors, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Authors'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aUuvpGj-Kul9"
   },
   "source": [
    "**Total number of paras for each author => the data is imbalace (milton only 29 paras)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZhdEHCkWKul-"
   },
   "source": [
    "split data to 20% test and 80% training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5dmfc-w2YvCF"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(data['Paras'], data['Authors'], test_size=0.2, random_state=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "wkirtx1cZOAN",
    "outputId": "2b0b4621-e1e9-4491-f5fe-295df5dceaf3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training shape: (38309,)(38309,)\n",
      "testing shape : (9578,)(9578,)\n"
     ]
    }
   ],
   "source": [
    "print(\"training shape: {}{}\".format(X_train.shape,y_train.shape))\n",
    "print(\"testing shape : {}{}\".format(X_test.shape,y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dgUBphoqY-_o"
   },
   "source": [
    "## Bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "K0r1wtXOZA8A",
    "outputId": "a4a5e05f-0e34-4799-d844-9ffd5ade8bac"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(38309, 5000)"
      ]
     },
     "execution_count": 19,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "count_vect = CountVectorizer(max_features = 5000)\n",
    "count_vect.fit(data['Paras'])\n",
    "X_train_counts = count_vect.transform(X_train)\n",
    "X_test_counts = count_vect.transform(X_test)\n",
    "X_train_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "x7edr5rZZpEV",
    "outputId": "bb776264-c14e-471b-a906-b57a25fce3f5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training shape: (38309, 5000)(38309,)\n",
      "testing shape : (9578, 5000)(9578,)\n"
     ]
    }
   ],
   "source": [
    "print(\"training shape: {}{}\".format(X_train_counts.shape,y_train.shape))\n",
    "print(\"testing shape : {}{}\".format(X_test_counts.shape,y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gKLbdQHSZ0uE"
   },
   "source": [
    "5000 Words in bag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Wx8pEPZhU_VP"
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(y_train)\n",
    "y_train_le = le.transform(y_train)\n",
    "\n",
    "le1 = preprocessing.LabelEncoder()\n",
    "le1.fit(y_test)\n",
    "y_test_le = le1.transform(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 485
    },
    "colab_type": "code",
    "id": "V9ihhQcMZ2wU",
    "outputId": "5b2cf21d-ad5a-411c-bc1f-f5ce6b2145b4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "                       max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=20,\n",
      "                       n_jobs=None, oob_score=False, random_state=1, verbose=0,\n",
      "                       warm_start=False)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.82      0.76      1003\n",
      "           1       0.93      0.98      0.96      4939\n",
      "           2       0.43      0.17      0.24        53\n",
      "           3       0.60      0.47      0.53       246\n",
      "           4       0.97      0.66      0.78        58\n",
      "           5       0.95      0.69      0.80       169\n",
      "           6       0.75      0.73      0.74       803\n",
      "           7       0.68      0.59      0.64       758\n",
      "           8       0.78      0.62      0.70       554\n",
      "           9       0.50      0.67      0.57         3\n",
      "          10       0.73      0.86      0.79       496\n",
      "          11       0.65      0.45      0.53       496\n",
      "\n",
      "    accuracy                           0.84      9578\n",
      "   macro avg       0.72      0.64      0.67      9578\n",
      "weighted avg       0.83      0.84      0.83      9578\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = RandomForestClassifier(n_estimators=20, random_state=1)\n",
    "print(model)\n",
    "model.fit(X_train_counts,y_train_le)\n",
    "pr = model.predict(X_test_counts)\n",
    "print(classification_report(y_test_le, pr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 199
    },
    "colab_type": "code",
    "id": "ANUlBB3schen",
    "outputId": "67b9e0c4-bd0b-4c50-90a8-d5c2be067b86"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(bootstrap=True, class_weight={0: 0.3, 1: 0.7},\n",
      "                       criterion='gini', max_depth=300, max_features='auto',\n",
      "                       max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "                       min_impurity_split=None, min_samples_leaf=1,\n",
      "                       min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "                       n_estimators=120, n_jobs=None, oob_score=False,\n",
      "                       random_state=None, verbose=0, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "param_grid = {'n_estimators': [80, 100, 120],\n",
    "              'max_depth' : [200, 250, 300],\n",
    "              'class_weight' : [{0:0.1,1:0.9}, {0:0.2,1:0.8},{0:0.3,1:0.7}]}\n",
    "RDF_grid = GridSearchCV(estimator=RandomForestClassifier(),\n",
    "                          param_grid = param_grid,\n",
    "                          scoring=\"f1_micro\",\n",
    "                          cv=3,\n",
    "                          n_jobs = 5)\n",
    "tick = time.time()\n",
    "RDF_grid.fit(X_train_counts, y_train_le)\n",
    "tock = time.time()\n",
    "RDF_grid_best = RDF_grid.best_estimator_ #best estimator\n",
    "print(RDF_grid_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 359
    },
    "colab_type": "code",
    "id": "Rs-MAEK0XetL",
    "outputId": "74331f1a-f45d-4925-e964-61ea619ae075"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.77      0.79      1003\n",
      "           1       0.93      0.98      0.96      4939\n",
      "           2       0.54      0.13      0.21        53\n",
      "           3       0.68      0.47      0.55       246\n",
      "           4       1.00      0.74      0.85        58\n",
      "           5       0.97      0.73      0.83       169\n",
      "           6       0.76      0.78      0.77       803\n",
      "           7       0.74      0.68      0.71       758\n",
      "           8       0.81      0.68      0.74       554\n",
      "           9       0.67      0.67      0.67         3\n",
      "          10       0.90      0.82      0.86       496\n",
      "          11       0.47      0.61      0.53       496\n",
      "\n",
      "    accuracy                           0.85      9578\n",
      "   macro avg       0.77      0.67      0.71      9578\n",
      "weighted avg       0.85      0.85      0.85      9578\n",
      "\n"
     ]
    }
   ],
   "source": [
    "RDF_grid_best.fit(X_train_counts,y_train_le)\n",
    "pr = RDF_grid_best.predict(X_test_counts)\n",
    "print(classification_report(y_test_le, pr))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color= 'red'>**=> The accuaracy imrpoved from 83% to 85% for previous version (max_depth and n_estimators tuned 20 to 300 and 120 respectively, and the best result is 300 and 120. It means that the model has a chance to continuosly incrase accuaracy)**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 467
    },
    "colab_type": "code",
    "id": "5_zx55puaHGg",
    "outputId": "b542dbdc-5a21-4ec3-fdb7-eedd992ca212"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "                       max_features=None, max_leaf_nodes=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, presort=False,\n",
      "                       random_state=2, splitter='best')\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      austen       0.72      0.74      0.73      1003\n",
      "       bible       0.95      0.94      0.95      4939\n",
      "       blake       0.21      0.13      0.16        53\n",
      "      bryant       0.44      0.51      0.47       246\n",
      "     burgess       0.89      0.72      0.80        58\n",
      "     carroll       0.84      0.69      0.76       169\n",
      "  chesterton       0.66      0.66      0.66       803\n",
      "   edgeworth       0.59      0.59      0.59       758\n",
      "    melville       0.65      0.58      0.61       554\n",
      "      milton       0.25      0.33      0.29         3\n",
      " shakespeare       0.62      0.84      0.71       496\n",
      "     whitman       0.50      0.39      0.44       496\n",
      "\n",
      "    accuracy                           0.79      9578\n",
      "   macro avg       0.61      0.60      0.60      9578\n",
      "weighted avg       0.79      0.79      0.79      9578\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = DecisionTreeClassifier(random_state=2)\n",
    "print(model)\n",
    "model.fit(X_train_counts,y_train)\n",
    "pr = model.predict(X_test_counts)\n",
    "print(classification_report(y_test, pr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 125
    },
    "colab_type": "code",
    "id": "7VySUdFuR75h",
    "outputId": "1c8c7db3-ce78-4dd2-a622-e50f17b852d8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight={0: 0.3, 1: 0.7}, criterion='gini',\n",
       "                       max_depth=1000, max_features='auto', max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, presort=False,\n",
       "                       random_state=None, splitter='best')"
      ]
     },
     "execution_count": 26,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "param_grid = {'criterion' :['gini', 'entropy'],\n",
    "              'max_features': ['auto', 'log2'],\n",
    "              'max_depth' : [1000, 1200, 1400],\n",
    "              'class_weight' : [{0:0.1,1:0.9}, {0:0.2,1:0.8},{0:0.3,1:0.7}]\n",
    "             }\n",
    "DT_grid = GridSearchCV(estimator=DecisionTreeClassifier(),\n",
    "                          param_grid = param_grid,\n",
    "                          scoring=\"f1_micro\",\n",
    "                          cv=3,\n",
    "                          n_jobs = 15)\n",
    "tick = time.time()\n",
    "DT_grid.fit(X_train_counts, y_train_le)\n",
    "tock = time.time()\n",
    "DT_grid_best = DT_grid.best_estimator_ #best estimator\n",
    "DT_grid_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 359
    },
    "colab_type": "code",
    "id": "1wiF8N_yjj21",
    "outputId": "cd9b5304-7fdd-449f-d634-328cf66ce323"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.59      0.60      1003\n",
      "           1       0.91      0.92      0.91      4939\n",
      "           2       0.25      0.19      0.22        53\n",
      "           3       0.33      0.38      0.35       246\n",
      "           4       0.68      0.40      0.50        58\n",
      "           5       0.57      0.59      0.58       169\n",
      "           6       0.56      0.55      0.56       803\n",
      "           7       0.48      0.49      0.48       758\n",
      "           8       0.48      0.45      0.47       554\n",
      "           9       0.25      0.33      0.29         3\n",
      "          10       0.58      0.77      0.66       496\n",
      "          11       0.42      0.31      0.36       496\n",
      "\n",
      "    accuracy                           0.73      9578\n",
      "   macro avg       0.51      0.50      0.50      9578\n",
      "weighted avg       0.72      0.73      0.72      9578\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DT_grid_best.fit(X_train_counts,y_train_le)\n",
    "pr = DT_grid_best.predict(X_test_counts)\n",
    "print(classification_report(y_test_le, pr))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color= 'red'>**=> The accuaracy go down from 79% to 73%.**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 377
    },
    "colab_type": "code",
    "id": "PreExIqdcT_H",
    "outputId": "b297b27e-9bdf-4557-e883-c431a67a71df"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GaussianNB(priors=None, var_smoothing=1e-09)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      austen       0.90      0.78      0.84      1003\n",
      "       bible       0.99      0.91      0.95      4939\n",
      "       blake       0.09      0.40      0.15        53\n",
      "      bryant       0.39      0.50      0.44       246\n",
      "     burgess       0.29      0.71      0.41        58\n",
      "     carroll       0.41      0.65      0.50       169\n",
      "  chesterton       0.82      0.64      0.72       803\n",
      "   edgeworth       0.67      0.79      0.73       758\n",
      "    melville       0.77      0.67      0.72       554\n",
      "      milton       0.06      0.67      0.12         3\n",
      " shakespeare       0.82      0.88      0.85       496\n",
      "     whitman       0.44      0.59      0.50       496\n",
      "\n",
      "    accuracy                           0.81      9578\n",
      "   macro avg       0.56      0.68      0.58      9578\n",
      "weighted avg       0.86      0.81      0.83      9578\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "model = GaussianNB()\n",
    "print(model)\n",
    "model.fit(X_train_counts.toarray(), y_train)\n",
    "pr = model.predict(X_test_counts.toarray())\n",
    "print(classification_report(y_test, pr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "076DtkFTlYQm",
    "outputId": "99fb8133-d7f4-4504-97a7-12edb4a8213b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24.383885622024536"
      ]
     },
     "execution_count": 29,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tock - tick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 91
    },
    "colab_type": "code",
    "id": "7jj7X9WZeeog",
    "outputId": "71fa2b40-a7a7-4ad8-969d-e8848ff0994a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GaussianNB(priors=None, var_smoothing=1e-08)"
      ]
     },
     "execution_count": 30,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = {'var_smoothing' :[0.00000000001,0.00000000003,0.0000000001,0.0000000003,\n",
    "                                0.000000001,0.000000003,0.00000001]}\n",
    "GNB_grid = GridSearchCV(estimator=GaussianNB(),\n",
    "                          param_grid = param_grid,\n",
    "                          scoring=\"f1_micro\",\n",
    "                          cv=3,\n",
    "                          n_jobs = -1)\n",
    "tick = time.time()\n",
    "GNB_grid.fit(X_train_counts.toarray(), y_train_le)\n",
    "tock = time.time()\n",
    "GNB_grid_best = GNB_grid.best_estimator_ #best estimator\n",
    "GNB_grid_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 359
    },
    "colab_type": "code",
    "id": "zzARvYZAmEuf",
    "outputId": "6891456b-d624-4193-c7cd-f06d757376a7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.78      0.84      1003\n",
      "           1       0.99      0.93      0.96      4939\n",
      "           2       0.09      0.40      0.15        53\n",
      "           3       0.39      0.50      0.44       246\n",
      "           4       0.29      0.71      0.41        58\n",
      "           5       0.41      0.65      0.50       169\n",
      "           6       0.82      0.65      0.72       803\n",
      "           7       0.69      0.79      0.73       758\n",
      "           8       0.77      0.68      0.72       554\n",
      "           9       0.07      0.67      0.13         3\n",
      "          10       0.82      0.88      0.85       496\n",
      "          11       0.49      0.59      0.53       496\n",
      "\n",
      "    accuracy                           0.82      9578\n",
      "   macro avg       0.56      0.68      0.58      9578\n",
      "weighted avg       0.86      0.82      0.84      9578\n",
      "\n"
     ]
    }
   ],
   "source": [
    "GNB_grid_best.fit(X_train_counts.toarray(),y_train_le)\n",
    "pr = GNB_grid_best.predict(X_test_counts.toarray())\n",
    "print(classification_report(y_test_le, pr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color= 'red'>**=> The accuaracy does not change from previous version but it is better than default model up to 1%.**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 377
    },
    "colab_type": "code",
    "id": "Vm3nfwLLMfS4",
    "outputId": "645b9ac1-ffdd-4d25-c859-38082dedd735"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113.09358930587769\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      austen       0.90      0.72      0.80      1003\n",
      "       bible       0.77      1.00      0.87      4939\n",
      "       blake       0.39      0.13      0.20        53\n",
      "      bryant       0.86      0.49      0.63       246\n",
      "     burgess       0.96      0.78      0.86        58\n",
      "     carroll       0.94      0.74      0.83       169\n",
      "  chesterton       0.93      0.61      0.74       803\n",
      "   edgeworth       0.96      0.62      0.76       758\n",
      "    melville       0.90      0.69      0.78       554\n",
      "      milton       0.40      0.67      0.50         3\n",
      " shakespeare       0.98      0.73      0.84       496\n",
      "     whitman       0.84      0.30      0.44       496\n",
      "\n",
      "    accuracy                           0.82      9578\n",
      "   macro avg       0.82      0.62      0.69      9578\n",
      "weighted avg       0.84      0.82      0.80      9578\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tick = time.time()\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "model = GradientBoostingClassifier(random_state=1)\n",
    "model.fit(X_train_counts,y_train)\n",
    "pr = model.predict(X_test_counts)\n",
    "print(time.time() - tick)\n",
    "print(classification_report(y_test, pr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "unHxIgm1Kumr"
   },
   "source": [
    "<font color= 'red'>**=> The accuaracy does not change%.**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color= 'red'>**=> In previous version the best one is gradient bossting 82%. However, the best on in this version is decision tree which is up to 85%**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rKZg1sw9jhDV"
   },
   "source": [
    "some words like “the” will appear many times and their large counts will not be very meaningful in the encoded vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1Wb0ltkDkU-p"
   },
   "source": [
    "## TF-IDF\n",
    "\n",
    "An alternative is to calculate word frequencies, and by far the most popular method is called TF-IDF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cNEIqaYQly6z"
   },
   "source": [
    "Term Frequency: This summarizes how often a given word appears within a document.\n",
    "\n",
    "Inverse Document Frequency: This downscales words that appear a lot across documents.\n",
    "\n",
    "=> TF-IDF are word frequency scores that try to highlight words that are more interesting, e.g. frequent in a document but not across documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "CRxvEEPtkS37",
    "outputId": "7ef745e6-8352-4f0b-f601-433326cc11fc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(38309, 5000)"
      ]
     },
     "execution_count": 33,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# create the transform\n",
    "vectorizer = TfidfVectorizer(max_features= 5000)\n",
    "# tokenize and build vocab\n",
    "vectorizer.fit(data['Paras'])\n",
    "# summarize\n",
    "#print(vectorizer.vocabulary_)\n",
    "#print(vectorizer.idf_)\n",
    "# encode document\n",
    "X_train_counts = vectorizer.transform(X_train)\n",
    "X_test_counts = vectorizer.transform(X_test)\n",
    "# summarize encoded vector\n",
    "#print(vector.shape)\n",
    "#print(vector.toarray())\n",
    "X_train_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "63Ne-e3ppsIn",
    "outputId": "3417f115-8118-4d14-fbdc-1861f0277416"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training shape: (38309, 5000)(38309,)\n",
      "testing shape : (9578, 5000)(9578,)\n"
     ]
    }
   ],
   "source": [
    "print(\"training shape: {}{}\".format(X_train_counts.shape,y_train.shape))\n",
    "print(\"testing shape : {}{}\".format(X_test_counts.shape,y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 359
    },
    "colab_type": "code",
    "id": "eq90OS0IvnpA",
    "outputId": "b2a0f344-9fe1-42e3-a9b9-e8446b4db447"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      austen       0.73      0.83      0.78      1003\n",
      "       bible       0.92      0.99      0.95      4939\n",
      "       blake       1.00      0.02      0.04        53\n",
      "      bryant       0.77      0.42      0.54       246\n",
      "     burgess       1.00      0.74      0.85        58\n",
      "     carroll       0.97      0.67      0.79       169\n",
      "  chesterton       0.76      0.75      0.76       803\n",
      "   edgeworth       0.78      0.63      0.70       758\n",
      "    melville       0.90      0.63      0.74       554\n",
      "      milton       0.67      0.67      0.67         3\n",
      " shakespeare       0.95      0.80      0.87       496\n",
      "     whitman       0.49      0.61      0.54       496\n",
      "\n",
      "    accuracy                           0.85      9578\n",
      "   macro avg       0.83      0.65      0.69      9578\n",
      "weighted avg       0.85      0.85      0.84      9578\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = RandomForestClassifier(n_estimators=120,max_depth=300 ,random_state=1)\n",
    "model.fit(X_train_counts,y_train)\n",
    "pr = model.predict(X_test_counts)\n",
    "print(classification_report(y_test, pr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color= 'red'>**=> The accuaracy increases from 83% to 85%.**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 359
    },
    "colab_type": "code",
    "id": "r1sKTP-SvsnV",
    "outputId": "63ae97c8-e2f5-4076-827f-bea5c05de10e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      austen       0.72      0.72      0.72      1003\n",
      "       bible       0.94      0.95      0.95      4939\n",
      "       blake       0.18      0.09      0.12        53\n",
      "      bryant       0.48      0.46      0.47       246\n",
      "     burgess       0.71      0.69      0.70        58\n",
      "     carroll       0.86      0.70      0.77       169\n",
      "  chesterton       0.67      0.68      0.67       803\n",
      "   edgeworth       0.63      0.59      0.61       758\n",
      "    melville       0.69      0.59      0.64       554\n",
      "      milton       0.40      0.67      0.50         3\n",
      " shakespeare       0.66      0.78      0.71       496\n",
      "     whitman       0.46      0.44      0.45       496\n",
      "\n",
      "    accuracy                           0.80      9578\n",
      "   macro avg       0.62      0.61      0.61      9578\n",
      "weighted avg       0.80      0.80      0.80      9578\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = DecisionTreeClassifier( max_depth=1200,random_state=2)\n",
    "model.fit(X_train_counts,y_train)\n",
    "pr = model.predict(X_test_counts)\n",
    "classification_report(y_test, pr)\n",
    "print(classification_report(y_test, pr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color= 'red'>**=> The accuaracy increase from 79% to 80%.**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 359
    },
    "colab_type": "code",
    "id": "6QelInI9wJCM",
    "outputId": "2d8d9bc0-e439-4157-c35b-b182bcce5eed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      austen       0.85      0.82      0.84      1003\n",
      "       bible       0.99      0.92      0.96      4939\n",
      "       blake       0.16      0.34      0.22        53\n",
      "      bryant       0.37      0.49      0.42       246\n",
      "     burgess       0.17      0.69      0.27        58\n",
      "     carroll       0.41      0.62      0.49       169\n",
      "  chesterton       0.81      0.71      0.76       803\n",
      "   edgeworth       0.79      0.75      0.77       758\n",
      "    melville       0.68      0.72      0.70       554\n",
      "      milton       0.04      0.67      0.07         3\n",
      " shakespeare       0.87      0.87      0.87       496\n",
      "     whitman       0.54      0.56      0.55       496\n",
      "\n",
      "    accuracy                           0.83      9578\n",
      "   macro avg       0.56      0.68      0.58      9578\n",
      "weighted avg       0.86      0.83      0.84      9578\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "model = GaussianNB(var_smoothing=1e-08)\n",
    "model.fit(X_train_counts.toarray(), y_train)\n",
    "pr = model.predict(X_test_counts.toarray())\n",
    "print(classification_report(y_test, pr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color= 'red'>**=> The accuaracy does not change.**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 377
    },
    "colab_type": "code",
    "id": "jvhuq0Y7w28h",
    "outputId": "b4adc24f-2bcb-4be2-bc7c-93b96365c6db"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "272.351704120636\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      austen       0.88      0.74      0.80      1003\n",
      "       bible       0.77      1.00      0.87      4939\n",
      "       blake       0.25      0.11      0.16        53\n",
      "      bryant       0.80      0.56      0.66       246\n",
      "     burgess       0.98      0.79      0.88        58\n",
      "     carroll       0.88      0.73      0.80       169\n",
      "  chesterton       0.94      0.62      0.75       803\n",
      "   edgeworth       0.93      0.61      0.74       758\n",
      "    melville       0.95      0.65      0.77       554\n",
      "      milton       0.50      0.67      0.57         3\n",
      " shakespeare       0.99      0.73      0.84       496\n",
      "     whitman       0.84      0.27      0.41       496\n",
      "\n",
      "    accuracy                           0.81      9578\n",
      "   macro avg       0.81      0.62      0.69      9578\n",
      "weighted avg       0.83      0.81      0.80      9578\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tick = time.time()\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "model = GradientBoostingClassifier(random_state=1)\n",
    "model.fit(X_train_counts,y_train)\n",
    "pr = model.predict(X_test_counts)\n",
    "print(time.time() - tick)\n",
    "print(classification_report(y_test, pr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color= 'red'>**=> The accuaracy descrase from 83% to 81%.**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UQoxydluKunS"
   },
   "source": [
    "<font color= 'red'>**=> The highest accuaracy 83% for Naive's baye instead of gradient boosting of previous version.**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xVmVyR_Gx7Tf"
   },
   "source": [
    "## Word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JKzSL1_Kx22p"
   },
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "from string import punctuation\n",
    "punc = set(punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 233
    },
    "colab_type": "code",
    "id": "Qfrzd7JZbzug",
    "outputId": "b1796f3d-2726-4173-d8a2-18f0800f2e9b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                     [[, Emma, by, Jane, Austen, 1816, ]]\n",
       "1                                              [VOLUME, I]\n",
       "2                                             [CHAPTER, I]\n",
       "3        [Emma, Woodhouse, ,, handsome, ,, clever, ,, a...\n",
       "4        [She, was, the, youngest, of, the, two, daught...\n",
       "                               ...                        \n",
       "47882                      [}, Good, -, Bye, My, Fancy, !]\n",
       "47883    [Good, -, bye, my, Fancy, !, Farewell, dear, m...\n",
       "47884    [Now, for, my, last, --, let, me, look, back, ...\n",
       "47885    [Long, have, we, lived, ,, joy, ', d, ,, cares...\n",
       "47886    [Yet, let, me, not, be, too, hasty, ,, Long, i...\n",
       "Name: Paras, Length: 47887, dtype: object"
      ]
     },
     "execution_count": 40,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataWV = dataOrig['Paras'].copy()\n",
    "dataWV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 233
    },
    "colab_type": "code",
    "id": "4IlGiMtcyH-q",
    "outputId": "715e09be-c72f-474c-9fea-b3b1fe5e7537"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                       [emma, jane, 1816]\n",
       "1                                                 [volume]\n",
       "2                                                [chapter]\n",
       "3        [emma, woodhouse, handsome, clever, rich, comf...\n",
       "4        [youngest, two, daughters, affectionate, indul...\n",
       "                               ...                        \n",
       "47882                                   [good, bye, fancy]\n",
       "47883    [good, bye, fancy, farewell, dear, mate, dear,...\n",
       "47884    [last, --, let, look, back, moment, slower, fa...\n",
       "47885    [long, lived, joy, caress, together, delightfu...\n",
       "47886    [yet, let, hasty, long, indeed, lived, slept, ...\n",
       "Name: Paras, Length: 47887, dtype: object"
      ]
     },
     "execution_count": 41,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english') + Authors + list(string.punctuation))\n",
    "for i in range(len(dataWV)):\n",
    "  words = []\n",
    "  for w in dataWV[i]:\n",
    "    if not w.lower() in stop_words :\n",
    "      words.append(w.lower()) \n",
    "  dataWV[i] = words\n",
    "dataWV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EdzRILIlCdoA"
   },
   "outputs": [],
   "source": [
    "sz = 300 # 500\n",
    "modelW2V = Word2Vec(dataWV, size=sz, window=600, min_count=10, workers=10, iter=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color= 'red'>**=> size, window, mincount and iter are already tuning after many tries**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "T4EH7hktCe8V"
   },
   "outputs": [],
   "source": [
    "#model.wv.most_similar(positive=\"girl\", topn =3)\n",
    "#len(model.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qIFaA-ZUGwLm"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "data_train, data_test, y_train, y_test = train_test_split(dataWV, dataOrig['Authors'], test_size=0.2, random_state=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5Z5a6rRojFNe"
   },
   "outputs": [],
   "source": [
    "import numpy as np  # Make sure that numpy is imported\n",
    "\n",
    "def makeFeatureVec(words, model, num_features):\n",
    "    # Function to average all of the word vectors in a given\n",
    "    # paragraph\n",
    "    #\n",
    "    # Pre-initialize an empty numpy array (for speed)\n",
    "    featureVec = np.zeros((num_features,),dtype=\"float32\")\n",
    "    #\n",
    "    nwords = 0.\n",
    "    # \n",
    "    # Index2word is a list that contains the names of the words in \n",
    "    # the model's vocabulary. Convert it to a set, for speed \n",
    "    index2word_set = set(model.wv.index2word)\n",
    "    #\n",
    "    # Loop over each word in the review and, if it is in the model's\n",
    "    # vocaublary, add its feature vector to the total\n",
    "    for word in words:\n",
    "        if word in index2word_set: \n",
    "            nwords = nwords + 1.\n",
    "            featureVec = np.add(featureVec,model[word])\n",
    "    # \n",
    "    # Divide the result by the number of words to get the average\n",
    "    featureVec = np.divide(featureVec,nwords)\n",
    "    return featureVec\n",
    "\n",
    "\n",
    "def getAvgFeatureVecs(reviews, model, num_features):\n",
    "    # Given a set of reviews (each one a list of words), calculate \n",
    "    # the average feature vector for each one and return a 2D numpy array \n",
    "    # \n",
    "    # Initialize a counter\n",
    "    counter = 0.\n",
    "    # \n",
    "    # Preallocate a 2D numpy array, for speed\n",
    "    reviewFeatureVecs = np.zeros((len(reviews),num_features),dtype=\"float32\")\n",
    "    # \n",
    "    # Loop through the reviews\n",
    "    for review in reviews:\n",
    "       #\n",
    "       # Print a status message every 1000th review\n",
    "       if counter%10000. == 0.:\n",
    "           print (\"Review {} of {}\" .format(counter, len(reviews)))\n",
    "\n",
    "       # Call the function (defined above) that makes average feature vectors\n",
    "       reviewFeatureVecs[int(counter)] = makeFeatureVec(review, model, num_features)\n",
    "       #\n",
    "       # Increment the counter\n",
    "       counter = counter + 1.\n",
    "    return reviewFeatureVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 163
    },
    "colab_type": "code",
    "id": "0lcApcpAl8m4",
    "outputId": "ed5eb452-059d-4139-e5c9-0b40d64547a4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 0.0 of 38309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:21: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:24: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 10000.0 of 38309\n",
      "Review 20000.0 of 38309\n",
      "Review 30000.0 of 38309\n",
      "Review 0.0 of 9578\n"
     ]
    }
   ],
   "source": [
    "X_train = getAvgFeatureVecs(data_train, modelW2V, sz)\n",
    "X_test = getAvgFeatureVecs(data_test, modelW2V, sz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RtesbmyhsUwL"
   },
   "outputs": [],
   "source": [
    "X_train = np.nan_to_num(X_train) \n",
    "X_test = np.nan_to_num(X_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 377
    },
    "colab_type": "code",
    "id": "GrYAk5yuntzu",
    "outputId": "3acde3bb-68ea-4236-94d4-975817fc6c74"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      austen       0.80      0.87      0.84      1003\n",
      "       bible       0.96      1.00      0.98      4939\n",
      "       blake       0.83      0.09      0.17        53\n",
      "      bryant       0.78      0.35      0.48       246\n",
      "     burgess       0.98      0.69      0.81        58\n",
      "     carroll       0.94      0.79      0.86       169\n",
      "  chesterton       0.77      0.86      0.81       803\n",
      "   edgeworth       0.67      0.68      0.68       758\n",
      "    melville       0.83      0.71      0.77       554\n",
      "      milton       0.17      0.33      0.22         3\n",
      " shakespeare       0.96      0.84      0.90       496\n",
      "     whitman       0.72      0.71      0.71       496\n",
      "\n",
      "    accuracy                           0.88      9578\n",
      "   macro avg       0.79      0.66      0.69      9578\n",
      "weighted avg       0.88      0.88      0.88      9578\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "148.56853532791138"
      ]
     },
     "execution_count": 48,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tick = time.time()\n",
    "#n_estimators=120,max_depth=300\n",
    "model = RandomForestClassifier(n_estimators=120,max_depth=300 , random_state=1)\n",
    "model.fit(X_train,y_train)\n",
    "pr = model.predict(X_test)\n",
    "print(classification_report(y_test, pr))\n",
    "time.time() - tick"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color= 'red'>**=> The accuaracy increase from 79% to 88% compared to previous version.**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 359
    },
    "colab_type": "code",
    "id": "hsnRDKOGsNgo",
    "outputId": "eb76a314-66f2-4695-f9c5-a5cb6892ffd2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      austen       0.69      0.73      0.71      1003\n",
      "       bible       0.97      0.96      0.96      4939\n",
      "       blake       0.15      0.17      0.16        53\n",
      "      bryant       0.32      0.34      0.33       246\n",
      "     burgess       0.60      0.62      0.61        58\n",
      "     carroll       0.67      0.59      0.63       169\n",
      "  chesterton       0.62      0.65      0.63       803\n",
      "   edgeworth       0.50      0.48      0.49       758\n",
      "    melville       0.58      0.60      0.59       554\n",
      "      milton       0.27      1.00      0.43         3\n",
      " shakespeare       0.78      0.75      0.76       496\n",
      "     whitman       0.47      0.46      0.47       496\n",
      "\n",
      "    accuracy                           0.78      9578\n",
      "   macro avg       0.55      0.61      0.56      9578\n",
      "weighted avg       0.79      0.78      0.79      9578\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = DecisionTreeClassifier(max_depth=1200,random_state=2)\n",
    "model.fit(X_train,y_train)\n",
    "pr = model.predict(X_test)\n",
    "classification_report(y_test, pr)\n",
    "print(classification_report(y_test, pr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color= 'red'>**=> The accuaracy increase from 72% to 78% compared to previous version.**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 359
    },
    "colab_type": "code",
    "id": "rUACfDmpsw4Z",
    "outputId": "ee621bc1-5a6f-46a6-f4cb-a6bf3444dec0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      austen       0.78      0.76      0.77      1003\n",
      "       bible       0.99      0.95      0.97      4939\n",
      "       blake       0.09      0.47      0.15        53\n",
      "      bryant       0.36      0.39      0.37       246\n",
      "     burgess       0.36      0.84      0.51        58\n",
      "     carroll       0.87      0.83      0.85       169\n",
      "  chesterton       0.68      0.75      0.71       803\n",
      "   edgeworth       0.58      0.54      0.56       758\n",
      "    melville       0.73      0.65      0.69       554\n",
      "      milton       0.19      1.00      0.32         3\n",
      " shakespeare       0.94      0.81      0.87       496\n",
      "     whitman       0.62      0.63      0.62       496\n",
      "\n",
      "    accuracy                           0.82      9578\n",
      "   macro avg       0.60      0.72      0.62      9578\n",
      "weighted avg       0.84      0.82      0.83      9578\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "model = GaussianNB(var_smoothing=1e-08)\n",
    "pr = model.fit(X_train, y_train)\n",
    "pr = model.predict(X_test)\n",
    "print(classification_report(y_test, pr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color= 'red'>**=> The accuaracy increase from 65% to 82% compared to previous version.**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 377
    },
    "colab_type": "code",
    "id": "CmnwH5Jst3on",
    "outputId": "62d7d2c0-05ae-413b-c238-cb2490ba57be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1948.2940249443054\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      austen       0.82      0.85      0.83      1003\n",
      "       bible       0.98      0.99      0.99      4939\n",
      "       blake       0.35      0.21      0.26        53\n",
      "      bryant       0.67      0.50      0.57       246\n",
      "     burgess       0.81      0.67      0.74        58\n",
      "     carroll       0.89      0.84      0.86       169\n",
      "  chesterton       0.79      0.83      0.81       803\n",
      "   edgeworth       0.69      0.68      0.68       758\n",
      "    melville       0.79      0.75      0.77       554\n",
      "      milton       0.14      0.33      0.20         3\n",
      " shakespeare       0.94      0.85      0.89       496\n",
      "     whitman       0.69      0.74      0.72       496\n",
      "\n",
      "    accuracy                           0.88      9578\n",
      "   macro avg       0.71      0.69      0.69      9578\n",
      "weighted avg       0.88      0.88      0.88      9578\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tick = time.time()\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "model = GradientBoostingClassifier(random_state=1)\n",
    "model.fit(X_train,y_train)\n",
    "pr = model.predict(X_test)\n",
    "print(time.time() - tick)\n",
    "print(classification_report(y_test, pr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S2wO__6SveAJ"
   },
   "source": [
    "<font color= 'red'>**=> The accuaracy increase from 80% to 88% compared to previous version.**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7VEVWct42Ie0"
   },
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 685
    },
    "colab_type": "code",
    "id": "jutbNv202Rmw",
    "outputId": "bffdfa45-37dd-4a67-ee31-593c9f4e0ef9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.6149249   1.2944665   0.9105315  ...  3.7473667   1.7258674\n",
      "  -0.26087084]\n",
      " [ 1.9661744  -1.0254343   3.1355164  ...  1.4374299  -1.9938434\n",
      "   0.04409772]\n",
      " [-0.7695444   0.11796419 -0.59558165 ... -0.13437365 -0.32220805\n",
      "  -0.0491445 ]\n",
      " ...\n",
      " [ 0.38510218 -0.00832579  0.18448576 ...  0.06716139  0.17125045\n",
      "  -0.00822563]\n",
      " [-0.28247282  0.15865894 -0.83941036 ... -0.06695636  0.21947138\n",
      "   0.05091806]\n",
      " [ 0.23323424  0.4981548  -0.74384993 ... -0.75298643 -0.38904727\n",
      "   0.8003735 ]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster id labels for inputted data\n",
      "[0 3 1 ... 1 1 1]\n",
      "Centroids data\n",
      "[[-0.6647718   0.64872146 -0.7813641  ...  0.31891087  1.1888773\n",
      "  -0.28204998]\n",
      " [-0.18681288  0.06621408 -0.33785132 ... -0.05359645  0.0179157\n",
      "   0.05084041]\n",
      " [-0.20481342  0.05053772 -0.20976686 ...  0.07560717  0.320293\n",
      "  -0.12872763]\n",
      " ...\n",
      " [-0.72423553  0.9641554  -3.0884938  ...  0.02187857  0.4182794\n",
      "   0.42999017]\n",
      " [-2.9566374   0.8212727  -3.3931105  ... -0.68058574  3.9126797\n",
      "  -0.24639897]\n",
      " [-1.1135857   0.11213609 -1.7273328  ...  0.10066728  0.95346\n",
      "  -0.0821972 ]]\n",
      "Score (Opposite of the value of X on the K-means objective which is Sum of distances of samples to their closest cluster center):\n",
      "-521983.94\n",
      "Silhouette_score: \n",
      "-0.013519149\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10.319035530090332"
      ]
     },
     "execution_count": 52,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    " \n",
    "from nltk.cluster import KMeansClusterer\n",
    " \n",
    "from sklearn import cluster\n",
    "from sklearn import metrics\n",
    " \n",
    "\n",
    "tick = time.time() \n",
    "# get vector data\n",
    "X = modelW2V[modelW2V.wv.vocab]\n",
    "print (X)\n",
    "\n",
    " \n",
    " \n",
    "NUM_CLUSTERS = 12\n",
    "kmeans = cluster.KMeans(n_clusters=NUM_CLUSTERS)\n",
    "kmeans.fit(X)\n",
    "labels = kmeans.labels_\n",
    "centroids = kmeans.cluster_centers_\n",
    " \n",
    "print (\"Cluster id labels for inputted data\")\n",
    "print (labels)\n",
    "print (\"Centroids data\")\n",
    "print (centroids)\n",
    " \n",
    "print (\"Score (Opposite of the value of X on the K-means objective which is Sum of distances of samples to their closest cluster center):\")\n",
    "print (kmeans.score(X))\n",
    " \n",
    "silhouette_score = metrics.silhouette_score(X, labels, metric='euclidean')\n",
    " \n",
    "print (\"Silhouette_score: \")\n",
    "print (silhouette_score)\n",
    "time.time() - tick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 455
    },
    "colab_type": "code",
    "id": "46dJ-Z0F73To",
    "outputId": "c6c8b6b8-aa1b-4b72-e075-ce93b1a85691"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10103\n",
      "10103\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>emma</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>jane</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>volume</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>chapter</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>woodhouse</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10098</th>\n",
       "      <td>brooklyn</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10099</th>\n",
       "      <td>growths</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10100</th>\n",
       "      <td>bugles</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10101</th>\n",
       "      <td>myriad</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10102</th>\n",
       "      <td>vigil</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10103 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           words  labels\n",
       "0           emma       0\n",
       "1           jane       3\n",
       "2         volume       1\n",
       "3        chapter       2\n",
       "4      woodhouse       3\n",
       "...          ...     ...\n",
       "10098   brooklyn       1\n",
       "10099    growths       1\n",
       "10100     bugles       1\n",
       "10101     myriad       1\n",
       "10102      vigil       1\n",
       "\n",
       "[10103 rows x 2 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = list(modelW2V.wv.vocab.keys())\n",
    "#labels\n",
    "print(len(words))\n",
    "print(len(labels))\n",
    "AuthorClass  = {\"words\": words,\n",
    "                \"labels\": labels}\n",
    "AuthorClass_df = pd.DataFrame(AuthorClass)    \n",
    "AuthorClass_df            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "colab_type": "code",
    "id": "_sY_6vXK-qHu",
    "outputId": "696bc81d-207e-4553-cb25-c76e08edf321"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>father</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>seven</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>374</th>\n",
       "      <td>shall</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>607</th>\n",
       "      <td>year</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>610</th>\n",
       "      <td>son</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6952</th>\n",
       "      <td>idols</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7273</th>\n",
       "      <td>jerusalem</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7385</th>\n",
       "      <td>david</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7680</th>\n",
       "      <td>babylon</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8000</th>\n",
       "      <td>119</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>116 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          words  labels\n",
       "31       father       7\n",
       "188       seven       7\n",
       "374       shall       7\n",
       "607        year       7\n",
       "610         son       7\n",
       "...         ...     ...\n",
       "6952      idols       7\n",
       "7273  jerusalem       7\n",
       "7385      david       7\n",
       "7680    babylon       7\n",
       "8000        119       7\n",
       "\n",
       "[116 rows x 2 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AuthorClass_df[labels == 7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color= 'red'>**=>Clustering the most used words of 12 authors**</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## LSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HbncSZv9QFsb"
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(data['Paras'], data['Authors'], test_size=0.2, random_state=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XvGXYM6YuYE0"
   },
   "source": [
    "\n",
    "The challenge is that the matrix is very sparse (or high dimension) and noisy (or include lots of low frequency word). So truncated SVD is adopted to reduce dimension.\n",
    "\n",
    "\n",
    "\n",
    "The idea of SVD is finding the most valuable information and using lower dimension t to represent same thing. [ref](https://github.com/makcedward/nlp/blob/master/sample/nlp-lsa_lda.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "HvuJI2QTuE_Z",
    "outputId": "62b068ce-3b86-426f-b596-7880bb6c5e94"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF output shape: (38309, 38604)\n",
      "LSA output shape: (38309, 10)\n",
      "Sum of explained variance ratio: 3%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "def build_lsa(x_train, x_test, dim=10):\n",
    "    tfidf_vec = TfidfVectorizer(use_idf=True, norm='l2')\n",
    "    svd = TruncatedSVD(n_components=dim)\n",
    "    \n",
    "    transformed_x_train = tfidf_vec.fit_transform(x_train)\n",
    "    transformed_x_test = tfidf_vec.transform(x_test)\n",
    "    \n",
    "    print('TF-IDF output shape:', transformed_x_train.shape)\n",
    "    \n",
    "    x_train_svd = svd.fit_transform(transformed_x_train)\n",
    "    x_test_svd = svd.transform(transformed_x_test)\n",
    "    \n",
    "    print('LSA output shape:', x_train_svd.shape)\n",
    "    \n",
    "    explained_variance = svd.explained_variance_ratio_.sum()\n",
    "    print(\"Sum of explained variance ratio: %d%%\" % (int(explained_variance * 100)))\n",
    "    \n",
    "    return tfidf_vec, svd, tfidf_vec.get_feature_names() , x_train_svd, x_test_svd\n",
    "\n",
    "def display_word_distribution_lsa(model, feature_names, n_word):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic %d:\" % (topic_idx))\n",
    "        words = []\n",
    "        for i in topic.argsort()[:-n_word - 1:-1]:\n",
    "            words.append(feature_names[i])\n",
    "        print(words)\n",
    "\n",
    "tfidf_vec, svd, feature_names, x_train_lsa, x_test_lsa = build_lsa(X_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 377
    },
    "colab_type": "code",
    "id": "yTPwldNIBogr",
    "outputId": "a5ff186b-b3d5-4f71-be6c-78ba3ca68541"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "['shall', 'unto', 'lord', 'thou', 'thy', 'said', 'thee', 'god', 'ye', 'man']\n",
      "Topic 1:\n",
      "['thou', 'thy', 'thee', 'shalt', 'hast', 'thine', 'unto', 'art', 'god', 'lord']\n",
      "Topic 2:\n",
      "['said', 'would', 'could', 'one', 'little', 'mr', 'man', 'well', 'know', 'mrs']\n",
      "Topic 3:\n",
      "['unto', 'lord', 'ye', 'israel', 'god', 'king', 'children', 'moses', 'came', 'hath']\n",
      "Topic 4:\n",
      "['said', 'ye', 'unto', 'thou', 'shall', 'go', 'thee', 'jesus', 'answered', 'say']\n",
      "Topic 5:\n",
      "['ye', 'god', 'thou', 'thy', 'know', 'would', 'us', 'christ', 'good', 'things']\n",
      "Topic 6:\n",
      "['chapter', 'thou', 'ye', 'shalt', '13', '22', 'came', '21', 'son', 'went']\n",
      "Topic 7:\n",
      "['lord', 'said', 'thy', 'chapter', 'god', 'thee', 'hath', 'let', 'us', 'know']\n",
      "Topic 8:\n",
      "['thy', 'thee', 'ye', 'king', 'unto', '11', 'come', 'son', 'came', '119']\n",
      "Topic 9:\n",
      "['king', 'said', 'ye', 'thy', 'son', 'thou', 'david', 'israel', 'house', 'judah']\n"
     ]
    }
   ],
   "source": [
    "display_word_distribution_lsa(model=svd, feature_names=feature_names, n_word=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<font color= 'red'>\n",
    "    \n",
    "**=> Topic 5: maybe somethhing about Jesus topic**\n",
    "    \n",
    "**=> Topic 3: about kingdoom topic**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 377
    },
    "colab_type": "code",
    "id": "l0MxHrCz3tmz",
    "outputId": "6a802e30-1a2a-4ffe-82c2-23149299b06f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "['blood', 'offering', 'without', 'congregation', 'burnt', 'kill', 'master', 'chief', 'wait', 'receive']\n",
      "Topic 1:\n",
      "['could', 'must', 'mrs', 'would', 'said', 'might', 'much', 'one', 'miss', 'without']\n",
      "Topic 2:\n",
      "['sake', 'enter', 'enemy', 'grass', '44', 'built', 'prepared', 'edge', 'moab', 'sick']\n",
      "Topic 3:\n",
      "['whale', 'hundred', 'ahab', 'twenty', 'three', 'boat', 'thousand', 'sea', 'whales', 'length']\n",
      "Topic 4:\n",
      "['shall', 'unto', 'lord', 'thou', 'thy', 'god', 'thee', 'ye', 'said', 'king']\n",
      "Topic 5:\n",
      "['34', 'glory', 'chapter', '40', 'judgment', 'places', 'disciples', 'song', 'vain', 'mountain']\n",
      "Topic 6:\n",
      "['fast', 'appearance', 'arthur', 'joab', 'armed', 'perceive', 'perceived', 'wounded', 'fir', 'exeunt']\n",
      "Topic 7:\n",
      "['said', 'like', 'one', 'mr', 'see', 'little', 'would', 'well', 'old', 'man']\n",
      "Topic 8:\n",
      "['book', 'ham', 'haue', 'laid', 'worship', 'st', 'grey', 'names', 'ha', 'bones']\n",
      "Topic 9:\n",
      "['stones', 'silent', 'devil', 'sons', '119', 'box', 'justice', 'jonathan', 'forgotten', 'row']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "def build_lda(x_train, x_test, num_of_topic=10):\n",
    "    vec = CountVectorizer()\n",
    "    transformed_x_train = vec.fit_transform(x_train)\n",
    "    transformed_x_test = vec.transform(x_test)\n",
    "    feature_names = vec.get_feature_names()\n",
    "\n",
    "    lda = LatentDirichletAllocation(\n",
    "        n_components=num_of_topic, max_iter=5, \n",
    "        learning_method='online', random_state=0)\n",
    "    x_train_lda = lda.fit_transform(transformed_x_train)\n",
    "    x_test_lda =  lda.fit(transformed_x_test)\n",
    "\n",
    "    return lda, vec, feature_names, x_train_lda, x_test_lda\n",
    "\n",
    "def display_word_distribution(model, feature_names, n_word):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic %d:\" % (topic_idx))\n",
    "        words = []\n",
    "        for i in topic.argsort()[:-n_word - 1:-1]:\n",
    "            words.append(feature_names[i])\n",
    "        print(words)\n",
    "\n",
    "lda_model, vec, feature_names, x_train_lda,  x_test_lda= build_lda(X_train,X_test)\n",
    "display_word_distribution(\n",
    "    model=lda_model, feature_names=feature_names, \n",
    "    n_word=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color= 'red'>\n",
    "\n",
    "**=> Topic 0: violent topic**\n",
    "    \n",
    "**=> Topic 3: above age or somthing related to years, era**\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NMGwK55XKuol"
   },
   "source": [
    "\n",
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ACdidW3gKuol"
   },
   "source": [
    "**=>BagofWords: Gradient boosting and Randomforest give the best result over 80% of accuracy**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L2cb9NWsKuom"
   },
   "source": [
    "**=>TF-IDF: The accuracy of the best is still 83%, but accuaracy of Naive Bayes and Decision tree are improved when comparing to Bagofword**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FRK7TP58Kuon"
   },
   "source": [
    "**=>Word2Vec: The performance worse than the others NLP techniques, the best case is 80% while the worse one is 65%**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fKzDBLTZKuoo"
   },
   "source": [
    "**Still not implemnet LDA to show the top 10 words**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HveTaMzUKuoo"
   },
   "source": [
    "**=> Wait for next week to reference project of other peoples in class to improve and revise my project**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JlWLDpBBKuop"
   },
   "source": [
    "<font color= 'red'> Bag of words\n",
    "\n",
    "**=> In previous version the best one is gradient bossting 82%. However, the best on in this version is decision tree which is up to 85%**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color= 'red'> TF-IDF\n",
    "    \n",
    "**=> In previous version the best one is gradient bossting 82%. However, the best on in this version is decision tree which is up to 85%**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color= 'red'> Word2Vec\n",
    "\n",
    "**=> In previous version the best one is gradient bossting 80%. However, the best on in this version are decision tree and gradient boosting which is up to 88%**\n",
    "\n",
    "**=> This is also give the best result for all technique as we expect from the theory**\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color= 'red'>**=>Clustering the most used words of 12 authors**</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color= 'red'> For LSA\n",
    "    \n",
    "**=> Topic 5: maybe somethhing about Jesus topic**\n",
    "    \n",
    "**=> Topic 3: about kingdoom topic**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color= 'red'> For LDA\n",
    "\n",
    "**=> Topic 0: violent topic**\n",
    "    \n",
    "**=> Topic 3: above age or somthing related to years, era**\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Author.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
